{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_dict = {\"negative\" : 0, \"positive\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/REVISION DATASET_b.xlsx\"\n",
    "df = pd.read_excel(fp,sheet_name=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>expresses a pain point</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestamp_epochs</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>Type of Pain</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Second category</th>\n",
       "      <th>Third category</th>\n",
       "      <th>Fourth category</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emmanuel Olabode</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@ifemeetstech your pr team can help bridge the...</td>\n",
       "      <td>y</td>\n",
       "      <td>42176</td>\n",
       "      <td>1434841837</td>\n",
       "      <td>612397168788406016</td>\n",
       "      <td>/olabodeEO/status/612397168788406272</td>\n",
       "      <td>1955234486</td>\n",
       "      <td>olabodeEO</td>\n",
       "      <td>gap</td>\n",
       "      <td>Operational issues</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alviniecððâ¨</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mcdonalds really bein missing uhp people food ...</td>\n",
       "      <td>y</td>\n",
       "      <td>2014-09-01 23:25:17</td>\n",
       "      <td>1409613917</td>\n",
       "      <td>506583600599662592</td>\n",
       "      <td>/ohhamazing/status/506583600599662592</td>\n",
       "      <td>2723652417</td>\n",
       "      <td>ohhamazing</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>Product feature or quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bobby</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>if they thought that little of him, why was he...</td>\n",
       "      <td>y</td>\n",
       "      <td>43720</td>\n",
       "      <td>1568242832</td>\n",
       "      <td>1171921495309910016</td>\n",
       "      <td>/Bobbythegreat/status/1171921495309914112</td>\n",
       "      <td>33740752</td>\n",
       "      <td>Bobbythegreat</td>\n",
       "      <td>gap</td>\n",
       "      <td>Product feature or quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elgen Bodenstien</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>when towns have locally owned  business capita...</td>\n",
       "      <td>y</td>\n",
       "      <td>2019-09-05 23:34:22</td>\n",
       "      <td>1567726462</td>\n",
       "      <td>1169755684122086912</td>\n",
       "      <td>/bodenstien/status/1169755684122087424</td>\n",
       "      <td>1167585352464424960</td>\n",
       "      <td>bodenstien</td>\n",
       "      <td>walmart</td>\n",
       "      <td>Company's image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robyn</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@arma_vancouver health information in records ...</td>\n",
       "      <td>y</td>\n",
       "      <td>2019-01-23 22:43:08</td>\n",
       "      <td>1548283388</td>\n",
       "      <td>1088205518886190976</td>\n",
       "      <td>/RobynCBird/status/1088205518886191104</td>\n",
       "      <td>2206030555</td>\n",
       "      <td>RobynCBird</td>\n",
       "      <td>fitbit</td>\n",
       "      <td>Company's image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fullname  is_retweet  likes  replies  retweets  \\\n",
       "0     Emmanuel Olabode           0      1        1         1   \n",
       "1  Alviniecððâ¨           0      0        0         0   \n",
       "2                Bobby           0      0        0         0   \n",
       "3     Elgen Bodenstien           0      0        0         0   \n",
       "4                Robyn           0      3        1         1   \n",
       "\n",
       "                                                text expresses a pain point  \\\n",
       "0  @ifemeetstech your pr team can help bridge the...                      y   \n",
       "1  mcdonalds really bein missing uhp people food ...                      y   \n",
       "2  if they thought that little of him, why was he...                      y   \n",
       "3  when towns have locally owned  business capita...                      y   \n",
       "4  @arma_vancouver health information in records ...                      y   \n",
       "\n",
       "             timestamp  timestamp_epochs             tweet_id  \\\n",
       "0                42176        1434841837   612397168788406016   \n",
       "1  2014-09-01 23:25:17        1409613917   506583600599662592   \n",
       "2                43720        1568242832  1171921495309910016   \n",
       "3  2019-09-05 23:34:22        1567726462  1169755684122086912   \n",
       "4  2019-01-23 22:43:08        1548283388  1088205518886190976   \n",
       "\n",
       "                                   tweet_url              user_id  \\\n",
       "0       /olabodeEO/status/612397168788406272           1955234486   \n",
       "1      /ohhamazing/status/506583600599662592           2723652417   \n",
       "2  /Bobbythegreat/status/1171921495309914112             33740752   \n",
       "3     /bodenstien/status/1169755684122087424  1167585352464424960   \n",
       "4     /RobynCBird/status/1088205518886191104           2206030555   \n",
       "\n",
       "        username      BRAND                Type of Pain Subjectivity  \\\n",
       "0      olabodeEO        gap          Operational issues          NaN   \n",
       "1     ohhamazing  mcdonalds  Product feature or quality          NaN   \n",
       "2  Bobbythegreat        gap  Product feature or quality          NaN   \n",
       "3     bodenstien    walmart             Company's image          NaN   \n",
       "4     RobynCBird     fitbit             Company's image          NaN   \n",
       "\n",
       "  Second category Third category Fourth category   Dataset  \n",
       "0             NaN            NaN             NaN  Original  \n",
       "1             NaN            NaN             NaN  Original  \n",
       "2             NaN            NaN             NaN  Original  \n",
       "3             NaN            NaN             NaN  Original  \n",
       "4             NaN            NaN             NaN  Original  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\",\"expresses a pain point\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"expresses a pain point\"].apply(lambda x: encode_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    return p.clean(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expresses a pain point</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ifemeetstech your pr team can help bridge the...</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>your pr team can help bridge the communication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mcdonalds really bein missing uhp people food ...</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>mcdonalds really bein missing uhp people food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if they thought that little of him, why was he...</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>if they thought that little of him, why was he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when towns have locally owned  business capita...</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>when towns have locally owned business capital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@arma_vancouver health information in records ...</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>health information in records means vulnerabil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text expresses a pain point  \\\n",
       "0  @ifemeetstech your pr team can help bridge the...                      y   \n",
       "1  mcdonalds really bein missing uhp people food ...                      y   \n",
       "2  if they thought that little of him, why was he...                      y   \n",
       "3  when towns have locally owned  business capita...                      y   \n",
       "4  @arma_vancouver health information in records ...                      y   \n",
       "\n",
       "   target                                         clean_text  \n",
       "0       1  your pr team can help bridge the communication...  \n",
       "1       1  mcdonalds really bein missing uhp people food ...  \n",
       "2       1  if they thought that little of him, why was he...  \n",
       "3       1  when towns have locally owned business capital...  \n",
       "4       1  health information in records means vulnerabil...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3348\n",
       "1    1252\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/imdb/imdb_preprocessed_reviews.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in df.clean_review]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 4)\n",
      "TRAIN Dataset: (40000, 4)\n",
      "VALID Dataset: (5000, 4)\n",
      "TEST Dataset: (5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.5888561010360718\n",
      "Training Accuracy per 100 steps: 66.46039603960396\n",
      "Training Loss per 100 steps: 0.4279055595397949\n",
      "Training Accuracy per 100 steps: 78.23383084577114\n",
      "Training Loss per 100 steps: 0.36209636926651\n",
      "Training Accuracy per 100 steps: 82.72425249169436\n",
      "Training Loss per 100 steps: 0.3322804570198059\n",
      "Training Accuracy per 100 steps: 84.72568578553616\n",
      "Training Loss per 100 steps: 0.30821728706359863\n",
      "Training Accuracy per 100 steps: 86.17764471057885\n",
      "Training Loss per 100 steps: 0.28797537088394165\n",
      "Training Accuracy per 100 steps: 87.2504159733777\n",
      "Training Loss per 100 steps: 0.2817936837673187\n",
      "Training Accuracy per 100 steps: 87.660485021398\n",
      "Training Loss per 100 steps: 0.27608686685562134\n",
      "Training Accuracy per 100 steps: 88.10861423220973\n",
      "Training Loss per 100 steps: 0.2688271105289459\n",
      "Training Accuracy per 100 steps: 88.56825749167592\n",
      "Training Loss per 100 steps: 0.26465579867362976\n",
      "Training Accuracy per 100 steps: 88.94855144855144\n",
      "Training Loss per 100 steps: 0.26045557856559753\n",
      "Training Accuracy per 100 steps: 89.16893732970027\n",
      "Training Loss per 100 steps: 0.25754180550575256\n",
      "Training Accuracy per 100 steps: 89.31099084096586\n",
      "Training Loss per 100 steps: 0.2508992850780487\n",
      "Training Accuracy per 100 steps: 89.64258262874712\n",
      "Training Loss per 100 steps: 0.2510336935520172\n",
      "Training Accuracy per 100 steps: 89.68593861527481\n",
      "Training Loss per 100 steps: 0.24789029359817505\n",
      "Training Accuracy per 100 steps: 89.881745502998\n",
      "Training Loss per 100 steps: 0.2467227280139923\n",
      "Training Accuracy per 100 steps: 89.91255465334166\n",
      "Training Loss per 100 steps: 0.2430698722600937\n",
      "Training Accuracy per 100 steps: 90.09406231628454\n",
      "Training Loss per 100 steps: 0.24002638459205627\n",
      "Training Accuracy per 100 steps: 90.2623542476402\n",
      "Training Loss per 100 steps: 0.23677287995815277\n",
      "Training Accuracy per 100 steps: 90.39978958442924\n",
      "Training Loss per 100 steps: 0.23380815982818604\n",
      "Training Accuracy per 100 steps: 90.5672163918041\n",
      "Training Loss per 100 steps: 0.23056167364120483\n",
      "Training Accuracy per 100 steps: 90.73655402189434\n",
      "Training Loss per 100 steps: 0.22713463008403778\n",
      "Training Accuracy per 100 steps: 90.91322126306224\n",
      "Training Loss per 100 steps: 0.22670790553092957\n",
      "Training Accuracy per 100 steps: 90.94958713602782\n",
      "Training Loss per 100 steps: 0.22614292800426483\n",
      "Training Accuracy per 100 steps: 90.98812994585589\n",
      "Training Loss per 100 steps: 0.22406867146492004\n",
      "Training Accuracy per 100 steps: 91.08356657337065\n",
      "Training Loss per 100 steps: 0.22296585142612457\n",
      "Training Accuracy per 100 steps: 91.15724721261053\n",
      "Training Loss per 100 steps: 0.22252483665943146\n",
      "Training Accuracy per 100 steps: 91.18382080710848\n",
      "Training Loss per 100 steps: 0.22152242064476013\n",
      "Training Accuracy per 100 steps: 91.21295965726526\n",
      "Training Loss per 100 steps: 0.21889610588550568\n",
      "Training Accuracy per 100 steps: 91.32626680455016\n",
      "Training Loss per 100 steps: 0.2177228480577469\n",
      "Training Accuracy per 100 steps: 91.37370876374541\n",
      "Training Loss per 100 steps: 0.21623440086841583\n",
      "Training Accuracy per 100 steps: 91.4584005159626\n",
      "Training Loss per 100 steps: 0.216827392578125\n",
      "Training Accuracy per 100 steps: 91.42455482661668\n",
      "Training Loss per 100 steps: 0.21649396419525146\n",
      "Training Accuracy per 100 steps: 91.4382005452893\n",
      "Training Loss per 100 steps: 0.21555973589420319\n",
      "Training Accuracy per 100 steps: 91.47677153778301\n",
      "Training Loss per 100 steps: 0.2143455296754837\n",
      "Training Accuracy per 100 steps: 91.53099114538703\n",
      "Training Loss per 100 steps: 0.21281389892101288\n",
      "Training Accuracy per 100 steps: 91.60649819494584\n",
      "Training Loss per 100 steps: 0.21212458610534668\n",
      "Training Accuracy per 100 steps: 91.61713050526885\n",
      "Training Loss per 100 steps: 0.21111661195755005\n",
      "Training Accuracy per 100 steps: 91.67324388318863\n",
      "Training Loss per 100 steps: 0.21050918102264404\n",
      "Training Accuracy per 100 steps: 91.68482440399897\n",
      "Training Loss per 100 steps: 0.21052032709121704\n",
      "Training Accuracy per 100 steps: 91.67083229192701\n",
      "Training Loss per 100 steps: 0.2106299251317978\n",
      "Training Accuracy per 100 steps: 91.65142648134601\n",
      "Training Loss per 100 steps: 0.2098998725414276\n",
      "Training Accuracy per 100 steps: 91.69542965960486\n",
      "Training Loss per 100 steps: 0.20907677710056305\n",
      "Training Accuracy per 100 steps: 91.72285514996513\n",
      "Training Loss per 100 steps: 0.20920078456401825\n",
      "Training Accuracy per 100 steps: 91.73483299250171\n",
      "Training Loss per 100 steps: 0.20855966210365295\n",
      "Training Accuracy per 100 steps: 91.7740502110642\n",
      "Training Loss per 100 steps: 0.20781925320625305\n",
      "Training Accuracy per 100 steps: 91.79797870028254\n",
      "Training Loss per 100 steps: 0.20679622888565063\n",
      "Training Accuracy per 100 steps: 91.84747925973197\n",
      "Training Loss per 100 steps: 0.20614206790924072\n",
      "Training Accuracy per 100 steps: 91.88971047698396\n",
      "Training Loss per 100 steps: 0.205768883228302\n",
      "Training Accuracy per 100 steps: 91.90726382370944\n",
      "The Total Accuracy for Epoch 0: 91.935\n",
      "Training Loss Epoch: 0.20546162128448486\n",
      "Training Accuracy Epoch: 91.935\n",
      "Validation Loss Epoch: 0.17248468101024628\n",
      "Validation Accuracy Epoch: 94.26\n",
      "Training Loss per 100 steps: 0.19137108325958252\n",
      "Training Accuracy per 100 steps: 93.1930693069307\n",
      "Training Loss per 100 steps: 0.16029243171215057\n",
      "Training Accuracy per 100 steps: 94.46517412935323\n",
      "Training Loss per 100 steps: 0.15665386617183685\n",
      "Training Accuracy per 100 steps: 94.35215946843854\n",
      "Training Loss per 100 steps: 0.15574823319911957\n",
      "Training Accuracy per 100 steps: 94.38902743142144\n",
      "Training Loss per 100 steps: 0.1484622210264206\n",
      "Training Accuracy per 100 steps: 94.56087824351297\n",
      "Training Loss per 100 steps: 0.141114741563797\n",
      "Training Accuracy per 100 steps: 94.82113144758735\n",
      "Training Loss per 100 steps: 0.14331334829330444\n",
      "Training Accuracy per 100 steps: 94.81098430813124\n",
      "Training Loss per 100 steps: 0.1431572139263153\n",
      "Training Accuracy per 100 steps: 94.8501872659176\n",
      "Training Loss per 100 steps: 0.1443777233362198\n",
      "Training Accuracy per 100 steps: 94.81132075471699\n",
      "Training Loss per 100 steps: 0.14711572229862213\n",
      "Training Accuracy per 100 steps: 94.68031968031968\n",
      "Training Loss per 100 steps: 0.14499862492084503\n",
      "Training Accuracy per 100 steps: 94.75476839237058\n",
      "Training Loss per 100 steps: 0.14483106136322021\n",
      "Training Accuracy per 100 steps: 94.7751873438801\n",
      "Training Loss per 100 steps: 0.14242376387119293\n",
      "Training Accuracy per 100 steps: 94.8597232897771\n",
      "Training Loss per 100 steps: 0.14400750398635864\n",
      "Training Accuracy per 100 steps: 94.81620271234833\n",
      "Training Loss per 100 steps: 0.1439044177532196\n",
      "Training Accuracy per 100 steps: 94.86175882744837\n",
      "Training Loss per 100 steps: 0.14470119774341583\n",
      "Training Accuracy per 100 steps: 94.77670206121174\n",
      "Training Loss per 100 steps: 0.14414283633232117\n",
      "Training Accuracy per 100 steps: 94.81922398589066\n",
      "Training Loss per 100 steps: 0.14242690801620483\n",
      "Training Accuracy per 100 steps: 94.88478622987229\n",
      "Training Loss per 100 steps: 0.14106088876724243\n",
      "Training Accuracy per 100 steps: 94.95002630194634\n",
      "Training Loss per 100 steps: 0.13950110971927643\n",
      "Training Accuracy per 100 steps: 94.94627686156922\n",
      "Training Loss per 100 steps: 0.13796210289001465\n",
      "Training Accuracy per 100 steps: 94.98453117563065\n",
      "Training Loss per 100 steps: 0.13750770688056946\n",
      "Training Accuracy per 100 steps: 94.99659245797365\n",
      "Training Loss per 100 steps: 0.13711674511432648\n",
      "Training Accuracy per 100 steps: 95.01847023033464\n",
      "Training Loss per 100 steps: 0.13723281025886536\n",
      "Training Accuracy per 100 steps: 94.99687630154102\n",
      "Training Loss per 100 steps: 0.13616067171096802\n",
      "Training Accuracy per 100 steps: 95.01199520191923\n",
      "Training Loss per 100 steps: 0.13613170385360718\n",
      "Training Accuracy per 100 steps: 95.01153402537486\n",
      "Training Loss per 100 steps: 0.13596446812152863\n",
      "Training Accuracy per 100 steps: 95.01110699740836\n",
      "Training Loss per 100 steps: 0.13551776111125946\n",
      "Training Accuracy per 100 steps: 95.03302392002855\n",
      "Training Loss per 100 steps: 0.13393059372901917\n",
      "Training Accuracy per 100 steps: 95.11375387797311\n",
      "Training Loss per 100 steps: 0.13353200256824493\n",
      "Training Accuracy per 100 steps: 95.134955014995\n",
      "Training Loss per 100 steps: 0.1331699639558792\n",
      "Training Accuracy per 100 steps: 95.12657207352467\n",
      "Training Loss per 100 steps: 0.13396450877189636\n",
      "Training Accuracy per 100 steps: 95.0952827241487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.13424207270145416\n",
      "Training Accuracy per 100 steps: 95.06588912450772\n",
      "Training Loss per 100 steps: 0.13404196500778198\n",
      "Training Accuracy per 100 steps: 95.06395177888857\n",
      "Training Loss per 100 steps: 0.1332867592573166\n",
      "Training Accuracy per 100 steps: 95.09068837475007\n",
      "Training Loss per 100 steps: 0.13205282390117645\n",
      "Training Accuracy per 100 steps: 95.14023882254929\n",
      "Training Loss per 100 steps: 0.13110433518886566\n",
      "Training Accuracy per 100 steps: 95.18373412591191\n",
      "Training Loss per 100 steps: 0.1305321902036667\n",
      "Training Accuracy per 100 steps: 95.20520915548539\n",
      "Training Loss per 100 steps: 0.13055989146232605\n",
      "Training Accuracy per 100 steps: 95.18072289156626\n",
      "Training Loss per 100 steps: 0.13078315556049347\n",
      "Training Accuracy per 100 steps: 95.18245438640339\n",
      "Training Loss per 100 steps: 0.13105717301368713\n",
      "Training Accuracy per 100 steps: 95.19324554986589\n",
      "Training Loss per 100 steps: 0.13077116012573242\n",
      "Training Accuracy per 100 steps: 95.20054748869318\n",
      "Training Loss per 100 steps: 0.1304740458726883\n",
      "Training Accuracy per 100 steps: 95.2104161822832\n",
      "Training Loss per 100 steps: 0.13062161207199097\n",
      "Training Accuracy per 100 steps: 95.19995455578278\n",
      "Training Loss per 100 steps: 0.1304764598608017\n",
      "Training Accuracy per 100 steps: 95.20662075094424\n",
      "Training Loss per 100 steps: 0.13018032908439636\n",
      "Training Accuracy per 100 steps: 95.22386437730928\n",
      "Training Loss per 100 steps: 0.128911092877388\n",
      "Training Accuracy per 100 steps: 95.27228249308658\n",
      "Training Loss per 100 steps: 0.1288178712129593\n",
      "Training Accuracy per 100 steps: 95.28744011664237\n",
      "Training Loss per 100 steps: 0.1289542317390442\n",
      "Training Accuracy per 100 steps: 95.28667618853295\n",
      "The Total Accuracy for Epoch 1: 95.3125\n",
      "Training Loss Epoch: 0.12848827242851257\n",
      "Training Accuracy Epoch: 95.3125\n",
      "Validation Loss Epoch: 0.21303129196166992\n",
      "Validation Accuracy Epoch: 94.3\n",
      "Training Loss per 100 steps: 0.13492247462272644\n",
      "Training Accuracy per 100 steps: 95.66831683168317\n",
      "Training Loss per 100 steps: 0.11552131175994873\n",
      "Training Accuracy per 100 steps: 96.45522388059702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5604f0f7def3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidating_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrn_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a0fd502eab2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, training_loader, testing_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbig_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalcuate_accu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-603b75fc39ac>\u001b[0m in \u001b[0;36mcalcuate_accu\u001b[0;34m(big_idx, targets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Function to calcuate the accuracy of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalcuate_accu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Epoch vs Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/models/best-ft-roberta-imdb-sentiment-maxlen256-bs8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2421,  117],\n",
       "       [ 126, 2336]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.14; Precision:95.2303302079087; Recall:94.88220958570268; F1-Score:95.05595116988809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.95      0.95      2538\n",
      "    positive       0.95      0.95      0.95      2462\n",
      "\n",
      "    accuracy                           0.95      5000\n",
      "   macro avg       0.95      0.95      0.95      5000\n",
      "weighted avg       0.95      0.95      0.95      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
