{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/imdb/imdb_preprocessed_reviews.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXwklEQVR4nO3df4zc9Z3f8efrIKAIkgIhXfkwV5PWOYlAS2AFSMlFm3IBQ05nUlUpCAWToDhRQE0kqp65VCIKTeVcLzmVKuXkXCxMleJDR3JYgZQ4KNP0pDrB5DiMIcQLMcKWg3WYg2xy4s65d/+Yz7YTZ9c7Ozu7tneeD2k033l/f8znvePdl78/ZiZVhSRJv3asByBJOj4YCJIkwECQJDUGgiQJMBAkSc3Jx3oAgzr77LNr1apVA637s5/9jNNOO224AzrOjWLPYN+jZBR7hvn3/fjjj/91Vb11pnknbCCsWrWKnTt3DrRup9NhYmJiuAM6zo1iz2Dfo2QUe4b5953khdnmzXnIKMm5Sb6T5Okku5N8stXPSrI9yZ52f2arJ8ldSSaTPJnk4p5trWvL70myrqd+SZJdbZ27kqTv7iRJQ9HPOYTDwG1VdT5wOXBLkvOBDcCjVbUaeLQ9BrgaWN1u64G7oRsgwB3AZcClwB3TIdKW+WjPemsW3pokaT7mDISqOlBVP2jTPwWeAc4B1gJb2mJbgGvb9Frg3uraAZyRZAVwFbC9qg5V1SvAdmBNm/fmqtpR3bdN39uzLUnSEpnXOYQkq4B3At8DxqrqQJv1E2CsTZ8DvNiz2r5WO1p93wz1mZ5/Pd29DsbGxuh0OvMZ/v8zNTU18LonqlHsGex7lIxizzDcvvsOhCSnAw8An6qq13oP81dVJVn0D0Wqqk3AJoDx8fEa9ATSKJ58GsWewb5HySj2DMPtu6/3ISR5A90w+GpVfa2VX2qHe2j3B1t9P3Buz+orW+1o9ZUz1CVJS6ifq4wCfAV4pqq+2DNrGzB9pdA64MGe+o3taqPLgVfboaVHgCuTnNlOJl8JPNLmvZbk8vZcN/ZsS5K0RPo5ZPQu4EPAriRPtNrvAxuB+5PcDLwAfLDNexi4BpgEfg58GKCqDiW5E3isLffZqjrUpj8B3AO8Efhmu0mSltCcgVBVfwHM9r6AK2ZYvoBbZtnWZmDzDPWdwAVzjUWStHhO2HcqL4VVGx7qa7m9G9+/yCORpMXnh9tJkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUtPPdypvTnIwyVM9tT9N8kS77Z3+as0kq5L8bc+8P+5Z55Iku5JMJrmrfX8ySc5Ksj3JnnZ/5iL0KUmaQz97CPcAa3oLVfVvquqiqroIeAD4Ws/s56bnVdXHe+p3Ax8FVrfb9DY3AI9W1Wrg0fZYkrTE5gyEqvoucGimee1/+R8E7jvaNpKsAN5cVTvady7fC1zbZq8FtrTpLT11SdISSvfv8xwLJauAb1TVBUfU3wN8sarGe5bbDfwIeA34D1X1v5OMAxur6rfbcr8F/F5V/U6Sv6mqM1o9wCvTj2cYx3pgPcDY2NglW7dunXfDAFNTU5x++ulzLrdr/6t9be/Cc/7RQONYSv32vNzY9+gYxZ5h/n2/973vfXz6b/aRTl7gWK7nl/cODgC/UVUvJ7kE+PMk7+h3Y1VVSWZNqKraBGwCGB8fr4mJiYEG3el06GfdmzY81Nf29t4w2DiWUr89Lzf2PTpGsWcYbt8DB0KSk4F/BVwyXauq14HX2/TjSZ4D3g7sB1b2rL6y1QBeSrKiqg60Q0sHBx2TJGlwC7ns9LeBH1bVvulCkrcmOalNv43uyePnq+oA8FqSy9thoRuBB9tq24B1bXpdT12StIT6uez0PuD/AL+ZZF+Sm9us6/jVk8nvAZ5sl6H+GfDxqpo+If0J4E+ASeA54JutvhF4X5I9dENm4+DtSJIGNecho6q6fpb6TTPUHqB7GepMy+8ELpih/jJwxVzjkCQtLt+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVLTz3cqb05yMMlTPbXPJNmf5Il2u6Zn3u1JJpM8m+SqnvqaVptMsqGnfl6S77X6nyY5ZZgNSpL6088ewj3Amhnqf1RVF7XbwwBJzgeuA97R1vlvSU5KchLwJeBq4Hzg+rYswOfbtv4Z8Apw80IakiQNZs5AqKrvAof63N5aYGtVvV5VPwYmgUvbbbKqnq+qvwO2AmuTBPiXwJ+19bcA186vBUnSMJy8gHVvTXIjsBO4rapeAc4BdvQss6/VAF48on4Z8Bbgb6rq8AzL/4ok64H1AGNjY3Q6nYEGPjU11de6t114eM5lgIHHsZT67Xm5se/RMYo9w3D7HjQQ7gbuBKrdfwH4yFBGdBRVtQnYBDA+Pl4TExMDbafT6dDPujdteKiv7e29YbBxLKV+e15u7Ht0jGLPMNy+BwqEqnppejrJl4FvtIf7gXN7Fl3ZasxSfxk4I8nJbS+hd3lJ0hIa6LLTJCt6Hn4AmL4CaRtwXZJTk5wHrAa+DzwGrG5XFJ1C98Tztqoq4DvAv27rrwMeHGRMkqSFmXMPIcl9wARwdpJ9wB3ARJKL6B4y2gt8DKCqdie5H3gaOAzcUlW/aNu5FXgEOAnYXFW721P8HrA1yX8E/hL4yrCakyT1b85AqKrrZyjP+ke7qj4HfG6G+sPAwzPUn6d7FZIk6RjyncqSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiSgj0BIsjnJwSRP9dT+c5IfJnkyydeTnNHqq5L8bZIn2u2Pe9a5JMmuJJNJ7kqSVj8ryfYke9r9mYvQpyRpDv3sIdwDrDmith24oKr+OfAj4Paeec9V1UXt9vGe+t3AR4HV7Ta9zQ3Ao1W1Gni0PZYkLbE5A6GqvgscOqL2rao63B7uAFYebRtJVgBvrqodVVXAvcC1bfZaYEub3tJTlyQtoWGcQ/gI8M2ex+cl+csk/yvJb7XaOcC+nmX2tRrAWFUdaNM/AcaGMCZJ0jydvJCVk3waOAx8tZUOAL9RVS8nuQT48yTv6Hd7VVVJ6ijPtx5YDzA2Nkan0xlo3FNTU32te9uFh+dcBhh4HEup356XG/seHaPYMwy374EDIclNwO8AV7TDQFTV68DrbfrxJM8Bbwf288uHlVa2GsBLSVZU1YF2aOngbM9ZVZuATQDj4+M1MTEx0Ng7nQ79rHvThof62t7eGwYbx1Lqt+flxr5Hxyj2DMPte6BDRknWAP8e+N2q+nlP/a1JTmrTb6N78vj5dkjotSSXt6uLbgQebKttA9a16XU9dUnSEppzDyHJfcAEcHaSfcAddK8qOhXY3q4e3dGuKHoP8Nkkfw/8A/Dxqpo+If0JulcsvZHuOYfp8w4bgfuT3Ay8AHxwKJ1JkuZlzkCoqutnKH9llmUfAB6YZd5O4IIZ6i8DV8w1DknS4vKdypIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiRggZ92qq5V/X4I3sb3L/JIJGlw7iFIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVLTVyAk2ZzkYJKnempnJdmeZE+7P7PVk+SuJJNJnkxycc8669rye5Ks66lfkmRXW+eutC9qliQtnX73EO4B1hxR2wA8WlWrgUfbY4CrgdXtth64G7oBAtwBXAZcCtwxHSJtmY/2rHfkc0mSFllfgVBV3wUOHVFeC2xp01uAa3vq91bXDuCMJCuAq4DtVXWoql4BtgNr2rw3V9WOqirg3p5tSZKWyEI+y2isqg606Z8AY236HODFnuX2tdrR6vtmqP+KJOvp7nUwNjZGp9MZaOBTU1N9rXvbhYcH2v5sBh3vMPTb83Jj36NjFHuG4fY9lA+3q6pKUsPY1hzPswnYBDA+Pl4TExMDbafT6dDPujf1+aF1/dp7w9zPuVj67Xm5se/RMYo9w3D7XshVRi+1wz20+4Otvh84t2e5la12tPrKGeqSpCW0kEDYBkxfKbQOeLCnfmO72uhy4NV2aOkR4MokZ7aTyVcCj7R5ryW5vF1ddGPPtiRJS6SvQ0ZJ7gMmgLOT7KN7tdBG4P4kNwMvAB9siz8MXANMAj8HPgxQVYeS3Ak81pb7bFVNn6j+BN0rmd4IfLPdJElLqK9AqKrrZ5l1xQzLFnDLLNvZDGyeob4TuKCfsUiSFofvVJYkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpGTgQkvxmkid6bq8l+VSSzyTZ31O/pmed25NMJnk2yVU99TWtNplkw0KbkiTNX1/fqTyTqnoWuAggyUnAfuDrwIeBP6qqP+xdPsn5wHXAO4BfB76d5O1t9peA9wH7gMeSbKuqpwcdmyRp/gYOhCNcATxXVS8kmW2ZtcDWqnod+HGSSeDSNm+yqp4HSLK1LWsgSNISGlYgXAfc1/P41iQ3AjuB26rqFeAcYEfPMvtaDeDFI+qXzfQkSdYD6wHGxsbodDoDDXZqaqqvdW+78PBA25/NoOMdhn57Xm7se3SMYs8w3L4XHAhJTgF+F7i9le4G7gSq3X8B+MhCnwegqjYBmwDGx8drYmJioO10Oh36WfemDQ8NtP3Z7L1h7udcLP32vNzY9+gYxZ5huH0PYw/hauAHVfUSwPQ9QJIvA99oD/cD5/ast7LVOEpdkrREhnHZ6fX0HC5KsqJn3geAp9r0NuC6JKcmOQ9YDXwfeAxYneS8trdxXVtWkrSEFrSHkOQ0ulcHfayn/AdJLqJ7yGjv9Lyq2p3kfroniw8Dt1TVL9p2bgUeAU4CNlfV7oWMS5I0fwsKhKr6GfCWI2ofOsrynwM+N0P9YeDhhYxFkrQwvlNZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMJyv0FSfVs3jO5r3bnz/Io5Ekn6VewiSJGAIgZBkb5JdSZ5IsrPVzkqyPcmedn9mqyfJXUkmkzyZ5OKe7axry+9Jsm6h45Ikzc+w9hDeW1UXVdV4e7wBeLSqVgOPtscAVwOr2209cDd0AwS4A7gMuBS4YzpEJElLY7EOGa0FtrTpLcC1PfV7q2sHcEaSFcBVwPaqOlRVrwDbgTWLNDZJ0gyGEQgFfCvJ40nWt9pYVR1o0z8Bxtr0OcCLPevua7XZ6pKkJTKMq4zeXVX7k/xjYHuSH/bOrKpKUkN4HlrgrAcYGxuj0+kMtJ2pqam+1r3twsMDbX8YBu1tNv32vNzY9+gYxZ5huH0vOBCqan+7P5jk63TPAbyUZEVVHWiHhA62xfcD5/asvrLV9gMTR9Q7MzzXJmATwPj4eE1MTBy5SF86nQ79rHvTPC4THba9N0wMdXv99rzc2PfoGMWeYbh9LygQkpwG/FpV/bRNXwl8FtgGrAM2tvsH2yrbgFuTbKV7AvnVFhqPAP+p50TylcDtCxnb0eza/+ox/WMvScejhe4hjAFfTzK9rf9RVf8zyWPA/UluBl4APtiWfxi4BpgEfg58GKCqDiW5E3isLffZqjq0wLFJkuZhQYFQVc8D/2KG+svAFTPUC7hllm1tBjYvZDySpMH5TmVJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBw/nGNC2CVX1+X8Peje9f5JFIGhXuIUiSAANBktQYCJIkwECQJDUDB0KSc5N8J8nTSXYn+WSrfybJ/iRPtNs1PevcnmQyybNJruqpr2m1ySQbFtaSJGkQC7nK6DBwW1X9IMmbgMeTbG/z/qiq/rB34STnA9cB7wB+Hfh2kre32V8C3gfsAx5Lsq2qnl7A2CRJ8zRwIFTVAeBAm/5pkmeAc46yylpga1W9Dvw4ySRwaZs3WVXPAyTZ2pY1ECRpCQ3lfQhJVgHvBL4HvAu4NcmNwE66exGv0A2LHT2r7eP/B8iLR9Qvm+V51gPrAcbGxuh0OgONd+yNcNuFhwda93jT789gampq4J/Xicy+R8co9gzD7XvBgZDkdOAB4FNV9VqSu4E7gWr3XwA+stDnAaiqTcAmgPHx8ZqYmBhoO//1qw/yhV3L4z15e2+Y6Gu5TqfDoD+vE5l9j45R7BmG2/eC/iomeQPdMPhqVX0NoKpe6pn/ZeAb7eF+4Nye1Ve2GkepS5KWyEKuMgrwFeCZqvpiT31Fz2IfAJ5q09uA65KcmuQ8YDXwfeAxYHWS85KcQvfE87ZBxyVJGsxC9hDeBXwI2JXkiVb7feD6JBfRPWS0F/gYQFXtTnI/3ZPFh4FbquoXAEluBR4BTgI2V9XuBYxLkjSAhVxl9BdAZpj18FHW+RzwuRnqDx9tPc2u3w/Bu2fNaYs8EkknOt+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkY0ofb6fi3a/+r3NTHexb2bnz/EoxG0vHIPQRJEmAgSJIaA0GSBBgIkqTGk8r6Jf1+WJ4nn6Xlxz0ESRJgIEiSGg8ZaSAeWpKWHwNBi8rgkE4cBoJOOIaMtDiOm0BIsgb4L3S/V/lPqmrjMR6SllC/f+QlLZ7jIhCSnAR8CXgfsA94LMm2qnr62I5MJ7JVGx7itgsPz/kZTu5JSF3Hy1VGlwKTVfV8Vf0dsBVYe4zHJEkj5bjYQwDOAV7sebwPuOzIhZKsB9a3h1NJnh3w+c4G/nrAdU9I/3YEe4b++s7nl2gwS2sUX+9R7Bnm3/c/mW3G8RIIfamqTcCmhW4nyc6qGh/CkE4Yo9gz2PexHsdSGsWeYbh9Hy+HjPYD5/Y8XtlqkqQlcrwEwmPA6iTnJTkFuA7YdozHJEkj5bg4ZFRVh5PcCjxC97LTzVW1exGfcsGHnU5Ao9gz2PcoGcWeYYh9p6qGtS1J0gnseDlkJEk6xgwESRIwYoGQZE2SZ5NMJtlwrMczbEn2JtmV5IkkO1vtrCTbk+xp92e2epLc1X4WTya5+NiOvn9JNic5mOSpntq8+0yyri2/J8m6Y9FLv2bp+TNJ9rfX+4kk1/TMu731/GySq3rqJ9TvQJJzk3wnydNJdif5ZKsv29f7KD0v/utdVSNxo3uy+jngbcApwF8B5x/rcQ25x73A2UfU/gDY0KY3AJ9v09cA3wQCXA5871iPfx59vge4GHhq0D6Bs4Dn2/2ZbfrMY93bPHv+DPDvZlj2/Pbv+1TgvPbv/qQT8XcAWAFc3KbfBPyo9bdsX++j9Lzor/co7SGM6sdjrAW2tOktwLU99XurawdwRpIVx2B881ZV3wUOHVGeb59XAdur6lBVvQJsB9Ys+uAHNEvPs1kLbK2q16vqx8Ak3X//J9zvQFUdqKoftOmfAs/Q/WSDZft6H6Xn2Qzt9R6lQJjp4zGO9kM+ERXwrSSPt4/5ABirqgNt+ifAWJtebj+P+fa5XPq/tR0a2Tx92IRl2nOSVcA7ge8xIq/3ET3DIr/eoxQIo+DdVXUxcDVwS5L39M6s7v7lsr/OeFT6BO4G/ilwEXAA+MIxHc0iSnI68ADwqap6rXfecn29Z+h50V/vUQqEZf/xGFW1v90fBL5Od5fxpelDQe3+YFt8uf085tvnCd9/Vb1UVb+oqn8Avkz39YZl1nOSN9D9w/jVqvpaKy/r13umnpfi9R6lQFjWH4+R5LQkb5qeBq4EnqLb4/QVFeuAB9v0NuDGdlXG5cCrPbvgJ6L59vkIcGWSM9uu95WtdsI44pzPB+i+3tDt+bokpyY5D1gNfJ8T8HcgSYCvAM9U1Rd7Zi3b13u2npfk9T7WZ9SX8kb3CoQf0T3z/uljPZ4h9/Y2ulcR/BWwe7o/4C3Ao8Ae4NvAWa0eul9K9BywCxg/1j3Mo9f76O4y/z3d46I3D9In8BG6J+AmgQ8f674G6Pm/t56ebL/oK3qW/3Tr+Vng6p76CfU7ALyb7uGgJ4En2u2a5fx6H6XnRX+9/egKSRIwWoeMJElHYSBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnN/wUK+ArV97j2BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in df.clean_review]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 4)\n",
      "TRAIN Dataset: (40000, 4)\n",
      "VALID Dataset: (5000, 4)\n",
      "TEST Dataset: (5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.5888561010360718\n",
      "Training Accuracy per 100 steps: 66.46039603960396\n",
      "Training Loss per 100 steps: 0.4279055595397949\n",
      "Training Accuracy per 100 steps: 78.23383084577114\n",
      "Training Loss per 100 steps: 0.36209636926651\n",
      "Training Accuracy per 100 steps: 82.72425249169436\n",
      "Training Loss per 100 steps: 0.3322804570198059\n",
      "Training Accuracy per 100 steps: 84.72568578553616\n",
      "Training Loss per 100 steps: 0.30821728706359863\n",
      "Training Accuracy per 100 steps: 86.17764471057885\n",
      "Training Loss per 100 steps: 0.28797537088394165\n",
      "Training Accuracy per 100 steps: 87.2504159733777\n",
      "Training Loss per 100 steps: 0.2817936837673187\n",
      "Training Accuracy per 100 steps: 87.660485021398\n",
      "Training Loss per 100 steps: 0.27608686685562134\n",
      "Training Accuracy per 100 steps: 88.10861423220973\n",
      "Training Loss per 100 steps: 0.2688271105289459\n",
      "Training Accuracy per 100 steps: 88.56825749167592\n",
      "Training Loss per 100 steps: 0.26465579867362976\n",
      "Training Accuracy per 100 steps: 88.94855144855144\n",
      "Training Loss per 100 steps: 0.26045557856559753\n",
      "Training Accuracy per 100 steps: 89.16893732970027\n",
      "Training Loss per 100 steps: 0.25754180550575256\n",
      "Training Accuracy per 100 steps: 89.31099084096586\n",
      "Training Loss per 100 steps: 0.2508992850780487\n",
      "Training Accuracy per 100 steps: 89.64258262874712\n",
      "Training Loss per 100 steps: 0.2510336935520172\n",
      "Training Accuracy per 100 steps: 89.68593861527481\n",
      "Training Loss per 100 steps: 0.24789029359817505\n",
      "Training Accuracy per 100 steps: 89.881745502998\n",
      "Training Loss per 100 steps: 0.2467227280139923\n",
      "Training Accuracy per 100 steps: 89.91255465334166\n",
      "Training Loss per 100 steps: 0.2430698722600937\n",
      "Training Accuracy per 100 steps: 90.09406231628454\n",
      "Training Loss per 100 steps: 0.24002638459205627\n",
      "Training Accuracy per 100 steps: 90.2623542476402\n",
      "Training Loss per 100 steps: 0.23677287995815277\n",
      "Training Accuracy per 100 steps: 90.39978958442924\n",
      "Training Loss per 100 steps: 0.23380815982818604\n",
      "Training Accuracy per 100 steps: 90.5672163918041\n",
      "Training Loss per 100 steps: 0.23056167364120483\n",
      "Training Accuracy per 100 steps: 90.73655402189434\n",
      "Training Loss per 100 steps: 0.22713463008403778\n",
      "Training Accuracy per 100 steps: 90.91322126306224\n",
      "Training Loss per 100 steps: 0.22670790553092957\n",
      "Training Accuracy per 100 steps: 90.94958713602782\n",
      "Training Loss per 100 steps: 0.22614292800426483\n",
      "Training Accuracy per 100 steps: 90.98812994585589\n",
      "Training Loss per 100 steps: 0.22406867146492004\n",
      "Training Accuracy per 100 steps: 91.08356657337065\n",
      "Training Loss per 100 steps: 0.22296585142612457\n",
      "Training Accuracy per 100 steps: 91.15724721261053\n",
      "Training Loss per 100 steps: 0.22252483665943146\n",
      "Training Accuracy per 100 steps: 91.18382080710848\n",
      "Training Loss per 100 steps: 0.22152242064476013\n",
      "Training Accuracy per 100 steps: 91.21295965726526\n",
      "Training Loss per 100 steps: 0.21889610588550568\n",
      "Training Accuracy per 100 steps: 91.32626680455016\n",
      "Training Loss per 100 steps: 0.2177228480577469\n",
      "Training Accuracy per 100 steps: 91.37370876374541\n",
      "Training Loss per 100 steps: 0.21623440086841583\n",
      "Training Accuracy per 100 steps: 91.4584005159626\n",
      "Training Loss per 100 steps: 0.216827392578125\n",
      "Training Accuracy per 100 steps: 91.42455482661668\n",
      "Training Loss per 100 steps: 0.21649396419525146\n",
      "Training Accuracy per 100 steps: 91.4382005452893\n",
      "Training Loss per 100 steps: 0.21555973589420319\n",
      "Training Accuracy per 100 steps: 91.47677153778301\n",
      "Training Loss per 100 steps: 0.2143455296754837\n",
      "Training Accuracy per 100 steps: 91.53099114538703\n",
      "Training Loss per 100 steps: 0.21281389892101288\n",
      "Training Accuracy per 100 steps: 91.60649819494584\n",
      "Training Loss per 100 steps: 0.21212458610534668\n",
      "Training Accuracy per 100 steps: 91.61713050526885\n",
      "Training Loss per 100 steps: 0.21111661195755005\n",
      "Training Accuracy per 100 steps: 91.67324388318863\n",
      "Training Loss per 100 steps: 0.21050918102264404\n",
      "Training Accuracy per 100 steps: 91.68482440399897\n",
      "Training Loss per 100 steps: 0.21052032709121704\n",
      "Training Accuracy per 100 steps: 91.67083229192701\n",
      "Training Loss per 100 steps: 0.2106299251317978\n",
      "Training Accuracy per 100 steps: 91.65142648134601\n",
      "Training Loss per 100 steps: 0.2098998725414276\n",
      "Training Accuracy per 100 steps: 91.69542965960486\n",
      "Training Loss per 100 steps: 0.20907677710056305\n",
      "Training Accuracy per 100 steps: 91.72285514996513\n",
      "Training Loss per 100 steps: 0.20920078456401825\n",
      "Training Accuracy per 100 steps: 91.73483299250171\n",
      "Training Loss per 100 steps: 0.20855966210365295\n",
      "Training Accuracy per 100 steps: 91.7740502110642\n",
      "Training Loss per 100 steps: 0.20781925320625305\n",
      "Training Accuracy per 100 steps: 91.79797870028254\n",
      "Training Loss per 100 steps: 0.20679622888565063\n",
      "Training Accuracy per 100 steps: 91.84747925973197\n",
      "Training Loss per 100 steps: 0.20614206790924072\n",
      "Training Accuracy per 100 steps: 91.88971047698396\n",
      "Training Loss per 100 steps: 0.205768883228302\n",
      "Training Accuracy per 100 steps: 91.90726382370944\n",
      "The Total Accuracy for Epoch 0: 91.935\n",
      "Training Loss Epoch: 0.20546162128448486\n",
      "Training Accuracy Epoch: 91.935\n",
      "Validation Loss Epoch: 0.17248468101024628\n",
      "Validation Accuracy Epoch: 94.26\n",
      "Training Loss per 100 steps: 0.19137108325958252\n",
      "Training Accuracy per 100 steps: 93.1930693069307\n",
      "Training Loss per 100 steps: 0.16029243171215057\n",
      "Training Accuracy per 100 steps: 94.46517412935323\n",
      "Training Loss per 100 steps: 0.15665386617183685\n",
      "Training Accuracy per 100 steps: 94.35215946843854\n",
      "Training Loss per 100 steps: 0.15574823319911957\n",
      "Training Accuracy per 100 steps: 94.38902743142144\n",
      "Training Loss per 100 steps: 0.1484622210264206\n",
      "Training Accuracy per 100 steps: 94.56087824351297\n",
      "Training Loss per 100 steps: 0.141114741563797\n",
      "Training Accuracy per 100 steps: 94.82113144758735\n",
      "Training Loss per 100 steps: 0.14331334829330444\n",
      "Training Accuracy per 100 steps: 94.81098430813124\n",
      "Training Loss per 100 steps: 0.1431572139263153\n",
      "Training Accuracy per 100 steps: 94.8501872659176\n",
      "Training Loss per 100 steps: 0.1443777233362198\n",
      "Training Accuracy per 100 steps: 94.81132075471699\n",
      "Training Loss per 100 steps: 0.14711572229862213\n",
      "Training Accuracy per 100 steps: 94.68031968031968\n",
      "Training Loss per 100 steps: 0.14499862492084503\n",
      "Training Accuracy per 100 steps: 94.75476839237058\n",
      "Training Loss per 100 steps: 0.14483106136322021\n",
      "Training Accuracy per 100 steps: 94.7751873438801\n",
      "Training Loss per 100 steps: 0.14242376387119293\n",
      "Training Accuracy per 100 steps: 94.8597232897771\n",
      "Training Loss per 100 steps: 0.14400750398635864\n",
      "Training Accuracy per 100 steps: 94.81620271234833\n",
      "Training Loss per 100 steps: 0.1439044177532196\n",
      "Training Accuracy per 100 steps: 94.86175882744837\n",
      "Training Loss per 100 steps: 0.14470119774341583\n",
      "Training Accuracy per 100 steps: 94.77670206121174\n",
      "Training Loss per 100 steps: 0.14414283633232117\n",
      "Training Accuracy per 100 steps: 94.81922398589066\n",
      "Training Loss per 100 steps: 0.14242690801620483\n",
      "Training Accuracy per 100 steps: 94.88478622987229\n",
      "Training Loss per 100 steps: 0.14106088876724243\n",
      "Training Accuracy per 100 steps: 94.95002630194634\n",
      "Training Loss per 100 steps: 0.13950110971927643\n",
      "Training Accuracy per 100 steps: 94.94627686156922\n",
      "Training Loss per 100 steps: 0.13796210289001465\n",
      "Training Accuracy per 100 steps: 94.98453117563065\n",
      "Training Loss per 100 steps: 0.13750770688056946\n",
      "Training Accuracy per 100 steps: 94.99659245797365\n",
      "Training Loss per 100 steps: 0.13711674511432648\n",
      "Training Accuracy per 100 steps: 95.01847023033464\n",
      "Training Loss per 100 steps: 0.13723281025886536\n",
      "Training Accuracy per 100 steps: 94.99687630154102\n",
      "Training Loss per 100 steps: 0.13616067171096802\n",
      "Training Accuracy per 100 steps: 95.01199520191923\n",
      "Training Loss per 100 steps: 0.13613170385360718\n",
      "Training Accuracy per 100 steps: 95.01153402537486\n",
      "Training Loss per 100 steps: 0.13596446812152863\n",
      "Training Accuracy per 100 steps: 95.01110699740836\n",
      "Training Loss per 100 steps: 0.13551776111125946\n",
      "Training Accuracy per 100 steps: 95.03302392002855\n",
      "Training Loss per 100 steps: 0.13393059372901917\n",
      "Training Accuracy per 100 steps: 95.11375387797311\n",
      "Training Loss per 100 steps: 0.13353200256824493\n",
      "Training Accuracy per 100 steps: 95.134955014995\n",
      "Training Loss per 100 steps: 0.1331699639558792\n",
      "Training Accuracy per 100 steps: 95.12657207352467\n",
      "Training Loss per 100 steps: 0.13396450877189636\n",
      "Training Accuracy per 100 steps: 95.0952827241487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.13424207270145416\n",
      "Training Accuracy per 100 steps: 95.06588912450772\n",
      "Training Loss per 100 steps: 0.13404196500778198\n",
      "Training Accuracy per 100 steps: 95.06395177888857\n",
      "Training Loss per 100 steps: 0.1332867592573166\n",
      "Training Accuracy per 100 steps: 95.09068837475007\n",
      "Training Loss per 100 steps: 0.13205282390117645\n",
      "Training Accuracy per 100 steps: 95.14023882254929\n",
      "Training Loss per 100 steps: 0.13110433518886566\n",
      "Training Accuracy per 100 steps: 95.18373412591191\n",
      "Training Loss per 100 steps: 0.1305321902036667\n",
      "Training Accuracy per 100 steps: 95.20520915548539\n",
      "Training Loss per 100 steps: 0.13055989146232605\n",
      "Training Accuracy per 100 steps: 95.18072289156626\n",
      "Training Loss per 100 steps: 0.13078315556049347\n",
      "Training Accuracy per 100 steps: 95.18245438640339\n",
      "Training Loss per 100 steps: 0.13105717301368713\n",
      "Training Accuracy per 100 steps: 95.19324554986589\n",
      "Training Loss per 100 steps: 0.13077116012573242\n",
      "Training Accuracy per 100 steps: 95.20054748869318\n",
      "Training Loss per 100 steps: 0.1304740458726883\n",
      "Training Accuracy per 100 steps: 95.2104161822832\n",
      "Training Loss per 100 steps: 0.13062161207199097\n",
      "Training Accuracy per 100 steps: 95.19995455578278\n",
      "Training Loss per 100 steps: 0.1304764598608017\n",
      "Training Accuracy per 100 steps: 95.20662075094424\n",
      "Training Loss per 100 steps: 0.13018032908439636\n",
      "Training Accuracy per 100 steps: 95.22386437730928\n",
      "Training Loss per 100 steps: 0.128911092877388\n",
      "Training Accuracy per 100 steps: 95.27228249308658\n",
      "Training Loss per 100 steps: 0.1288178712129593\n",
      "Training Accuracy per 100 steps: 95.28744011664237\n",
      "Training Loss per 100 steps: 0.1289542317390442\n",
      "Training Accuracy per 100 steps: 95.28667618853295\n",
      "The Total Accuracy for Epoch 1: 95.3125\n",
      "Training Loss Epoch: 0.12848827242851257\n",
      "Training Accuracy Epoch: 95.3125\n",
      "Validation Loss Epoch: 0.21303129196166992\n",
      "Validation Accuracy Epoch: 94.3\n",
      "Training Loss per 100 steps: 0.13492247462272644\n",
      "Training Accuracy per 100 steps: 95.66831683168317\n",
      "Training Loss per 100 steps: 0.11552131175994873\n",
      "Training Accuracy per 100 steps: 96.45522388059702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5604f0f7def3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidating_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrn_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a0fd502eab2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, training_loader, testing_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbig_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalcuate_accu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-603b75fc39ac>\u001b[0m in \u001b[0;36mcalcuate_accu\u001b[0;34m(big_idx, targets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Function to calcuate the accuracy of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalcuate_accu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Epoch vs Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/models/best-ft-roberta-imdb-sentiment-maxlen256-bs8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2421,  117],\n",
       "       [ 126, 2336]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.14; Precision:95.2303302079087; Recall:94.88220958570268; F1-Score:95.05595116988809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.95      0.95      2538\n",
      "    positive       0.95      0.95      0.95      2462\n",
      "\n",
      "    accuracy                           0.95      5000\n",
      "   macro avg       0.95      0.95      0.95      5000\n",
      "weighted avg       0.95      0.95      0.95      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
