{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/imdb/imdb_preprocessed_reviews.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXwklEQVR4nO3df4zc9Z3f8efrIKAIkgIhXfkwV5PWOYlAS2AFSMlFm3IBQ05nUlUpCAWToDhRQE0kqp65VCIKTeVcLzmVKuXkXCxMleJDR3JYgZQ4KNP0pDrB5DiMIcQLMcKWg3WYg2xy4s65d/+Yz7YTZ9c7Ozu7tneeD2k033l/f8znvePdl78/ZiZVhSRJv3asByBJOj4YCJIkwECQJDUGgiQJMBAkSc3Jx3oAgzr77LNr1apVA637s5/9jNNOO224AzrOjWLPYN+jZBR7hvn3/fjjj/91Vb11pnknbCCsWrWKnTt3DrRup9NhYmJiuAM6zo1iz2Dfo2QUe4b5953khdnmzXnIKMm5Sb6T5Okku5N8stXPSrI9yZ52f2arJ8ldSSaTPJnk4p5trWvL70myrqd+SZJdbZ27kqTv7iRJQ9HPOYTDwG1VdT5wOXBLkvOBDcCjVbUaeLQ9BrgaWN1u64G7oRsgwB3AZcClwB3TIdKW+WjPemsW3pokaT7mDISqOlBVP2jTPwWeAc4B1gJb2mJbgGvb9Frg3uraAZyRZAVwFbC9qg5V1SvAdmBNm/fmqtpR3bdN39uzLUnSEpnXOYQkq4B3At8DxqrqQJv1E2CsTZ8DvNiz2r5WO1p93wz1mZ5/Pd29DsbGxuh0OvMZ/v8zNTU18LonqlHsGex7lIxizzDcvvsOhCSnAw8An6qq13oP81dVJVn0D0Wqqk3AJoDx8fEa9ATSKJ58GsWewb5HySj2DMPtu6/3ISR5A90w+GpVfa2VX2qHe2j3B1t9P3Buz+orW+1o9ZUz1CVJS6ifq4wCfAV4pqq+2DNrGzB9pdA64MGe+o3taqPLgVfboaVHgCuTnNlOJl8JPNLmvZbk8vZcN/ZsS5K0RPo5ZPQu4EPAriRPtNrvAxuB+5PcDLwAfLDNexi4BpgEfg58GKCqDiW5E3isLffZqjrUpj8B3AO8Efhmu0mSltCcgVBVfwHM9r6AK2ZYvoBbZtnWZmDzDPWdwAVzjUWStHhO2HcqL4VVGx7qa7m9G9+/yCORpMXnh9tJkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUtPPdypvTnIwyVM9tT9N8kS77Z3+as0kq5L8bc+8P+5Z55Iku5JMJrmrfX8ySc5Ksj3JnnZ/5iL0KUmaQz97CPcAa3oLVfVvquqiqroIeAD4Ws/s56bnVdXHe+p3Ax8FVrfb9DY3AI9W1Wrg0fZYkrTE5gyEqvoucGimee1/+R8E7jvaNpKsAN5cVTvady7fC1zbZq8FtrTpLT11SdISSvfv8xwLJauAb1TVBUfU3wN8sarGe5bbDfwIeA34D1X1v5OMAxur6rfbcr8F/F5V/U6Sv6mqM1o9wCvTj2cYx3pgPcDY2NglW7dunXfDAFNTU5x++ulzLrdr/6t9be/Cc/7RQONYSv32vNzY9+gYxZ5h/n2/973vfXz6b/aRTl7gWK7nl/cODgC/UVUvJ7kE+PMk7+h3Y1VVSWZNqKraBGwCGB8fr4mJiYEG3el06GfdmzY81Nf29t4w2DiWUr89Lzf2PTpGsWcYbt8DB0KSk4F/BVwyXauq14HX2/TjSZ4D3g7sB1b2rL6y1QBeSrKiqg60Q0sHBx2TJGlwC7ns9LeBH1bVvulCkrcmOalNv43uyePnq+oA8FqSy9thoRuBB9tq24B1bXpdT12StIT6uez0PuD/AL+ZZF+Sm9us6/jVk8nvAZ5sl6H+GfDxqpo+If0J4E+ASeA54JutvhF4X5I9dENm4+DtSJIGNecho6q6fpb6TTPUHqB7GepMy+8ELpih/jJwxVzjkCQtLt+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVLTz3cqb05yMMlTPbXPJNmf5Il2u6Zn3u1JJpM8m+SqnvqaVptMsqGnfl6S77X6nyY5ZZgNSpL6088ewj3Amhnqf1RVF7XbwwBJzgeuA97R1vlvSU5KchLwJeBq4Hzg+rYswOfbtv4Z8Apw80IakiQNZs5AqKrvAof63N5aYGtVvV5VPwYmgUvbbbKqnq+qvwO2AmuTBPiXwJ+19bcA186vBUnSMJy8gHVvTXIjsBO4rapeAc4BdvQss6/VAF48on4Z8Bbgb6rq8AzL/4ok64H1AGNjY3Q6nYEGPjU11de6t114eM5lgIHHsZT67Xm5se/RMYo9w3D7HjQQ7gbuBKrdfwH4yFBGdBRVtQnYBDA+Pl4TExMDbafT6dDPujdteKiv7e29YbBxLKV+e15u7Ht0jGLPMNy+BwqEqnppejrJl4FvtIf7gXN7Fl3ZasxSfxk4I8nJbS+hd3lJ0hIa6LLTJCt6Hn4AmL4CaRtwXZJTk5wHrAa+DzwGrG5XFJ1C98Tztqoq4DvAv27rrwMeHGRMkqSFmXMPIcl9wARwdpJ9wB3ARJKL6B4y2gt8DKCqdie5H3gaOAzcUlW/aNu5FXgEOAnYXFW721P8HrA1yX8E/hL4yrCakyT1b85AqKrrZyjP+ke7qj4HfG6G+sPAwzPUn6d7FZIk6RjyncqSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiSgj0BIsjnJwSRP9dT+c5IfJnkyydeTnNHqq5L8bZIn2u2Pe9a5JMmuJJNJ7kqSVj8ryfYke9r9mYvQpyRpDv3sIdwDrDmith24oKr+OfAj4Paeec9V1UXt9vGe+t3AR4HV7Ta9zQ3Ao1W1Gni0PZYkLbE5A6GqvgscOqL2rao63B7uAFYebRtJVgBvrqodVVXAvcC1bfZaYEub3tJTlyQtoWGcQ/gI8M2ex+cl+csk/yvJb7XaOcC+nmX2tRrAWFUdaNM/AcaGMCZJ0jydvJCVk3waOAx8tZUOAL9RVS8nuQT48yTv6Hd7VVVJ6ijPtx5YDzA2Nkan0xlo3FNTU32te9uFh+dcBhh4HEup356XG/seHaPYMwy374EDIclNwO8AV7TDQFTV68DrbfrxJM8Bbwf288uHlVa2GsBLSVZU1YF2aOngbM9ZVZuATQDj4+M1MTEx0Ng7nQ79rHvThof62t7eGwYbx1Lqt+flxr5Hxyj2DMPte6BDRknWAP8e+N2q+nlP/a1JTmrTb6N78vj5dkjotSSXt6uLbgQebKttA9a16XU9dUnSEppzDyHJfcAEcHaSfcAddK8qOhXY3q4e3dGuKHoP8Nkkfw/8A/Dxqpo+If0JulcsvZHuOYfp8w4bgfuT3Ay8AHxwKJ1JkuZlzkCoqutnKH9llmUfAB6YZd5O4IIZ6i8DV8w1DknS4vKdypIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiRggZ92qq5V/X4I3sb3L/JIJGlw7iFIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVLTVyAk2ZzkYJKnempnJdmeZE+7P7PVk+SuJJNJnkxycc8669rye5Ks66lfkmRXW+eutC9qliQtnX73EO4B1hxR2wA8WlWrgUfbY4CrgdXtth64G7oBAtwBXAZcCtwxHSJtmY/2rHfkc0mSFllfgVBV3wUOHVFeC2xp01uAa3vq91bXDuCMJCuAq4DtVXWoql4BtgNr2rw3V9WOqirg3p5tSZKWyEI+y2isqg606Z8AY236HODFnuX2tdrR6vtmqP+KJOvp7nUwNjZGp9MZaOBTU1N9rXvbhYcH2v5sBh3vMPTb83Jj36NjFHuG4fY9lA+3q6pKUsPY1hzPswnYBDA+Pl4TExMDbafT6dDPujf1+aF1/dp7w9zPuVj67Xm5se/RMYo9w3D7XshVRi+1wz20+4Otvh84t2e5la12tPrKGeqSpCW0kEDYBkxfKbQOeLCnfmO72uhy4NV2aOkR4MokZ7aTyVcCj7R5ryW5vF1ddGPPtiRJS6SvQ0ZJ7gMmgLOT7KN7tdBG4P4kNwMvAB9siz8MXANMAj8HPgxQVYeS3Ak81pb7bFVNn6j+BN0rmd4IfLPdJElLqK9AqKrrZ5l1xQzLFnDLLNvZDGyeob4TuKCfsUiSFofvVJYkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpGTgQkvxmkid6bq8l+VSSzyTZ31O/pmed25NMJnk2yVU99TWtNplkw0KbkiTNX1/fqTyTqnoWuAggyUnAfuDrwIeBP6qqP+xdPsn5wHXAO4BfB76d5O1t9peA9wH7gMeSbKuqpwcdmyRp/gYOhCNcATxXVS8kmW2ZtcDWqnod+HGSSeDSNm+yqp4HSLK1LWsgSNISGlYgXAfc1/P41iQ3AjuB26rqFeAcYEfPMvtaDeDFI+qXzfQkSdYD6wHGxsbodDoDDXZqaqqvdW+78PBA25/NoOMdhn57Xm7se3SMYs8w3L4XHAhJTgF+F7i9le4G7gSq3X8B+MhCnwegqjYBmwDGx8drYmJioO10Oh36WfemDQ8NtP3Z7L1h7udcLP32vNzY9+gYxZ5huH0PYw/hauAHVfUSwPQ9QJIvA99oD/cD5/ast7LVOEpdkrREhnHZ6fX0HC5KsqJn3geAp9r0NuC6JKcmOQ9YDXwfeAxYneS8trdxXVtWkrSEFrSHkOQ0ulcHfayn/AdJLqJ7yGjv9Lyq2p3kfroniw8Dt1TVL9p2bgUeAU4CNlfV7oWMS5I0fwsKhKr6GfCWI2ofOsrynwM+N0P9YeDhhYxFkrQwvlNZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMJyv0FSfVs3jO5r3bnz/Io5Ekn6VewiSJGAIgZBkb5JdSZ5IsrPVzkqyPcmedn9mqyfJXUkmkzyZ5OKe7axry+9Jsm6h45Ikzc+w9hDeW1UXVdV4e7wBeLSqVgOPtscAVwOr2209cDd0AwS4A7gMuBS4YzpEJElLY7EOGa0FtrTpLcC1PfV7q2sHcEaSFcBVwPaqOlRVrwDbgTWLNDZJ0gyGEQgFfCvJ40nWt9pYVR1o0z8Bxtr0OcCLPevua7XZ6pKkJTKMq4zeXVX7k/xjYHuSH/bOrKpKUkN4HlrgrAcYGxuj0+kMtJ2pqam+1r3twsMDbX8YBu1tNv32vNzY9+gYxZ5huH0vOBCqan+7P5jk63TPAbyUZEVVHWiHhA62xfcD5/asvrLV9gMTR9Q7MzzXJmATwPj4eE1MTBy5SF86nQ79rHvTPC4THba9N0wMdXv99rzc2PfoGMWeYbh9LygQkpwG/FpV/bRNXwl8FtgGrAM2tvsH2yrbgFuTbKV7AvnVFhqPAP+p50TylcDtCxnb0eza/+ox/WMvScejhe4hjAFfTzK9rf9RVf8zyWPA/UluBl4APtiWfxi4BpgEfg58GKCqDiW5E3isLffZqjq0wLFJkuZhQYFQVc8D/2KG+svAFTPUC7hllm1tBjYvZDySpMH5TmVJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBw/nGNC2CVX1+X8Peje9f5JFIGhXuIUiSAANBktQYCJIkwECQJDUDB0KSc5N8J8nTSXYn+WSrfybJ/iRPtNs1PevcnmQyybNJruqpr2m1ySQbFtaSJGkQC7nK6DBwW1X9IMmbgMeTbG/z/qiq/rB34STnA9cB7wB+Hfh2kre32V8C3gfsAx5Lsq2qnl7A2CRJ8zRwIFTVAeBAm/5pkmeAc46yylpga1W9Dvw4ySRwaZs3WVXPAyTZ2pY1ECRpCQ3lfQhJVgHvBL4HvAu4NcmNwE66exGv0A2LHT2r7eP/B8iLR9Qvm+V51gPrAcbGxuh0OgONd+yNcNuFhwda93jT789gampq4J/Xicy+R8co9gzD7XvBgZDkdOAB4FNV9VqSu4E7gWr3XwA+stDnAaiqTcAmgPHx8ZqYmBhoO//1qw/yhV3L4z15e2+Y6Gu5TqfDoD+vE5l9j45R7BmG2/eC/iomeQPdMPhqVX0NoKpe6pn/ZeAb7eF+4Nye1Ve2GkepS5KWyEKuMgrwFeCZqvpiT31Fz2IfAJ5q09uA65KcmuQ8YDXwfeAxYHWS85KcQvfE87ZBxyVJGsxC9hDeBXwI2JXkiVb7feD6JBfRPWS0F/gYQFXtTnI/3ZPFh4FbquoXAEluBR4BTgI2V9XuBYxLkjSAhVxl9BdAZpj18FHW+RzwuRnqDx9tPc2u3w/Bu2fNaYs8EkknOt+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkY0ofb6fi3a/+r3NTHexb2bnz/EoxG0vHIPQRJEmAgSJIaA0GSBBgIkqTGk8r6Jf1+WJ4nn6Xlxz0ESRJgIEiSGg8ZaSAeWpKWHwNBi8rgkE4cBoJOOIaMtDiOm0BIsgb4L3S/V/lPqmrjMR6SllC/f+QlLZ7jIhCSnAR8CXgfsA94LMm2qnr62I5MJ7JVGx7itgsPz/kZTu5JSF3Hy1VGlwKTVfV8Vf0dsBVYe4zHJEkj5bjYQwDOAV7sebwPuOzIhZKsB9a3h1NJnh3w+c4G/nrAdU9I/3YEe4b++s7nl2gwS2sUX+9R7Bnm3/c/mW3G8RIIfamqTcCmhW4nyc6qGh/CkE4Yo9gz2PexHsdSGsWeYbh9Hy+HjPYD5/Y8XtlqkqQlcrwEwmPA6iTnJTkFuA7YdozHJEkj5bg4ZFRVh5PcCjxC97LTzVW1exGfcsGHnU5Ao9gz2PcoGcWeYYh9p6qGtS1J0gnseDlkJEk6xgwESRIwYoGQZE2SZ5NMJtlwrMczbEn2JtmV5IkkO1vtrCTbk+xp92e2epLc1X4WTya5+NiOvn9JNic5mOSpntq8+0yyri2/J8m6Y9FLv2bp+TNJ9rfX+4kk1/TMu731/GySq3rqJ9TvQJJzk3wnydNJdif5ZKsv29f7KD0v/utdVSNxo3uy+jngbcApwF8B5x/rcQ25x73A2UfU/gDY0KY3AJ9v09cA3wQCXA5871iPfx59vge4GHhq0D6Bs4Dn2/2ZbfrMY93bPHv+DPDvZlj2/Pbv+1TgvPbv/qQT8XcAWAFc3KbfBPyo9bdsX++j9Lzor/co7SGM6sdjrAW2tOktwLU99XurawdwRpIVx2B881ZV3wUOHVGeb59XAdur6lBVvQJsB9Ys+uAHNEvPs1kLbK2q16vqx8Ak3X//J9zvQFUdqKoftOmfAs/Q/WSDZft6H6Xn2Qzt9R6lQJjp4zGO9kM+ERXwrSSPt4/5ABirqgNt+ifAWJtebj+P+fa5XPq/tR0a2Tx92IRl2nOSVcA7ge8xIq/3ET3DIr/eoxQIo+DdVXUxcDVwS5L39M6s7v7lsr/OeFT6BO4G/ilwEXAA+MIxHc0iSnI68ADwqap6rXfecn29Z+h50V/vUQqEZf/xGFW1v90fBL5Od5fxpelDQe3+YFt8uf085tvnCd9/Vb1UVb+oqn8Avkz39YZl1nOSN9D9w/jVqvpaKy/r13umnpfi9R6lQFjWH4+R5LQkb5qeBq4EnqLb4/QVFeuAB9v0NuDGdlXG5cCrPbvgJ6L59vkIcGWSM9uu95WtdsI44pzPB+i+3tDt+bokpyY5D1gNfJ8T8HcgSYCvAM9U1Rd7Zi3b13u2npfk9T7WZ9SX8kb3CoQf0T3z/uljPZ4h9/Y2ulcR/BWwe7o/4C3Ao8Ae4NvAWa0eul9K9BywCxg/1j3Mo9f76O4y/z3d46I3D9In8BG6J+AmgQ8f674G6Pm/t56ebL/oK3qW/3Tr+Vng6p76CfU7ALyb7uGgJ4En2u2a5fx6H6XnRX+9/egKSRIwWoeMJElHYSBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnN/wUK+ArV97j2BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in df.clean_review]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 4)\n",
      "TRAIN Dataset: (40000, 4)\n",
      "VALID Dataset: (5000, 4)\n",
      "TEST Dataset: (5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6426449418067932\n",
      "Training Accuracy per 100 steps: 62.5\n",
      "Training Loss per 100 steps: 0.5229800939559937\n",
      "Training Accuracy per 100 steps: 73.2587064676617\n",
      "Training Loss per 100 steps: 0.45583340525627136\n",
      "Training Accuracy per 100 steps: 77.99003322259136\n",
      "Training Loss per 100 steps: 0.41374528408050537\n",
      "Training Accuracy per 100 steps: 80.54862842892769\n",
      "Training Loss per 100 steps: 0.38414910435676575\n",
      "Training Accuracy per 100 steps: 82.31037924151697\n",
      "Training Loss per 100 steps: 0.35943150520324707\n",
      "Training Accuracy per 100 steps: 83.65224625623961\n",
      "Training Loss per 100 steps: 0.3507402539253235\n",
      "Training Accuracy per 100 steps: 84.23680456490727\n",
      "Training Loss per 100 steps: 0.3437948226928711\n",
      "Training Accuracy per 100 steps: 84.75343320848938\n",
      "Training Loss per 100 steps: 0.3362765312194824\n",
      "Training Accuracy per 100 steps: 85.1276359600444\n",
      "Training Loss per 100 steps: 0.33249059319496155\n",
      "Training Accuracy per 100 steps: 85.35214785214785\n",
      "Training Loss per 100 steps: 0.32489851117134094\n",
      "Training Accuracy per 100 steps: 85.78564940962761\n",
      "Training Loss per 100 steps: 0.31889358162879944\n",
      "Training Accuracy per 100 steps: 86.1157368859284\n",
      "Training Loss per 100 steps: 0.31248244643211365\n",
      "Training Accuracy per 100 steps: 86.55841660261338\n",
      "Training Loss per 100 steps: 0.3112483322620392\n",
      "Training Accuracy per 100 steps: 86.56316916488223\n",
      "Training Loss per 100 steps: 0.3064446747303009\n",
      "Training Accuracy per 100 steps: 86.80879413724183\n",
      "Training Loss per 100 steps: 0.3049539029598236\n",
      "Training Accuracy per 100 steps: 86.85977514053717\n",
      "Training Loss per 100 steps: 0.3015749752521515\n",
      "Training Accuracy per 100 steps: 87.08112874779542\n",
      "Training Loss per 100 steps: 0.2983914017677307\n",
      "Training Accuracy per 100 steps: 87.21543586896169\n",
      "Training Loss per 100 steps: 0.2940697968006134\n",
      "Training Accuracy per 100 steps: 87.40136770120989\n",
      "Training Loss per 100 steps: 0.2899855673313141\n",
      "Training Accuracy per 100 steps: 87.57496251874063\n",
      "Training Loss per 100 steps: 0.2872597277164459\n",
      "Training Accuracy per 100 steps: 87.7201332698715\n",
      "Training Loss per 100 steps: 0.2848682403564453\n",
      "Training Accuracy per 100 steps: 87.89754656974102\n",
      "Training Loss per 100 steps: 0.2839552164077759\n",
      "Training Accuracy per 100 steps: 87.96175575836592\n",
      "Training Loss per 100 steps: 0.2836347818374634\n",
      "Training Accuracy per 100 steps: 87.9789670970429\n",
      "Training Loss per 100 steps: 0.28175967931747437\n",
      "Training Accuracy per 100 steps: 88.06977209116353\n",
      "Training Loss per 100 steps: 0.2799818217754364\n",
      "Training Accuracy per 100 steps: 88.17281814686659\n",
      "Training Loss per 100 steps: 0.2787574529647827\n",
      "Training Accuracy per 100 steps: 88.22195483154387\n",
      "Training Loss per 100 steps: 0.2776055932044983\n",
      "Training Accuracy per 100 steps: 88.28543377365227\n",
      "Training Loss per 100 steps: 0.2741883099079132\n",
      "Training Accuracy per 100 steps: 88.46087556015168\n",
      "Training Loss per 100 steps: 0.27295249700546265\n",
      "Training Accuracy per 100 steps: 88.52049316894369\n",
      "Training Loss per 100 steps: 0.2714521884918213\n",
      "Training Accuracy per 100 steps: 88.62060625604644\n",
      "Training Loss per 100 steps: 0.27141523361206055\n",
      "Training Accuracy per 100 steps: 88.63636363636364\n",
      "Training Loss per 100 steps: 0.27067795395851135\n",
      "Training Accuracy per 100 steps: 88.6511663132384\n",
      "Training Loss per 100 steps: 0.2697180211544037\n",
      "Training Accuracy per 100 steps: 88.68347544839753\n",
      "Training Loss per 100 steps: 0.2684108316898346\n",
      "Training Accuracy per 100 steps: 88.7317909168809\n",
      "Training Loss per 100 steps: 0.2663487493991852\n",
      "Training Accuracy per 100 steps: 88.83990558178284\n",
      "Training Loss per 100 steps: 0.2653839588165283\n",
      "Training Accuracy per 100 steps: 88.87800594433936\n",
      "Training Loss per 100 steps: 0.2646133303642273\n",
      "Training Accuracy per 100 steps: 88.91739016048409\n",
      "Training Loss per 100 steps: 0.26330435276031494\n",
      "Training Accuracy per 100 steps: 88.9675724173289\n",
      "Training Loss per 100 steps: 0.2631130814552307\n",
      "Training Accuracy per 100 steps: 88.98400399900025\n",
      "Training Loss per 100 steps: 0.262334942817688\n",
      "Training Accuracy per 100 steps: 89.02401853206536\n",
      "Training Loss per 100 steps: 0.2617393434047699\n",
      "Training Accuracy per 100 steps: 89.07402999285884\n",
      "Training Loss per 100 steps: 0.2606818675994873\n",
      "Training Accuracy per 100 steps: 89.14496628691002\n",
      "Training Loss per 100 steps: 0.2606203258037567\n",
      "Training Accuracy per 100 steps: 89.14735287434674\n",
      "Training Loss per 100 steps: 0.25985246896743774\n",
      "Training Accuracy per 100 steps: 89.1801821817374\n",
      "Training Loss per 100 steps: 0.2596592903137207\n",
      "Training Accuracy per 100 steps: 89.19256683329711\n",
      "Training Loss per 100 steps: 0.2585054337978363\n",
      "Training Accuracy per 100 steps: 89.24696873005743\n",
      "Training Loss per 100 steps: 0.2580114006996155\n",
      "Training Accuracy per 100 steps: 89.2756717350552\n",
      "Training Loss per 100 steps: 0.2579938769340515\n",
      "Training Accuracy per 100 steps: 89.27769842889207\n",
      "The Total Accuracy for Epoch 0: 89.31\n",
      "Training Loss Epoch: 0.2570643126964569\n",
      "Training Accuracy Epoch: 89.31\n",
      "Validation Loss Epoch: 0.2064359039068222\n",
      "Validation Accuracy Epoch: 92.0\n",
      "Training Loss per 100 steps: 0.21803995966911316\n",
      "Training Accuracy per 100 steps: 93.06930693069307\n",
      "Training Loss per 100 steps: 0.1931033879518509\n",
      "Training Accuracy per 100 steps: 93.47014925373135\n",
      "Training Loss per 100 steps: 0.18528470396995544\n",
      "Training Accuracy per 100 steps: 93.56312292358804\n",
      "Training Loss per 100 steps: 0.17853695154190063\n",
      "Training Accuracy per 100 steps: 93.54738154613466\n",
      "Training Loss per 100 steps: 0.1675034910440445\n",
      "Training Accuracy per 100 steps: 93.812375249501\n",
      "Training Loss per 100 steps: 0.16029150784015656\n",
      "Training Accuracy per 100 steps: 94.03078202995009\n",
      "Training Loss per 100 steps: 0.16228348016738892\n",
      "Training Accuracy per 100 steps: 94.02639087018545\n",
      "Training Loss per 100 steps: 0.16338953375816345\n",
      "Training Accuracy per 100 steps: 93.9294631710362\n",
      "Training Loss per 100 steps: 0.16321562230587006\n",
      "Training Accuracy per 100 steps: 93.88179800221975\n",
      "Training Loss per 100 steps: 0.1640632003545761\n",
      "Training Accuracy per 100 steps: 93.94355644355645\n",
      "Training Loss per 100 steps: 0.16133198142051697\n",
      "Training Accuracy per 100 steps: 94.05086285195277\n",
      "Training Loss per 100 steps: 0.15936397016048431\n",
      "Training Accuracy per 100 steps: 94.12989175686927\n",
      "Training Loss per 100 steps: 0.157282292842865\n",
      "Training Accuracy per 100 steps: 94.18716372021522\n",
      "Training Loss per 100 steps: 0.15929573774337769\n",
      "Training Accuracy per 100 steps: 94.14703783012135\n",
      "Training Loss per 100 steps: 0.15860331058502197\n",
      "Training Accuracy per 100 steps: 94.15389740173218\n",
      "Training Loss per 100 steps: 0.15873919427394867\n",
      "Training Accuracy per 100 steps: 94.14428482198626\n",
      "Training Loss per 100 steps: 0.15836107730865479\n",
      "Training Accuracy per 100 steps: 94.17989417989418\n",
      "Training Loss per 100 steps: 0.1561853289604187\n",
      "Training Accuracy per 100 steps: 94.24625208217657\n",
      "Training Loss per 100 steps: 0.15478579699993134\n",
      "Training Accuracy per 100 steps: 94.33193056286166\n",
      "Training Loss per 100 steps: 0.15294893085956573\n",
      "Training Accuracy per 100 steps: 94.35907046476761\n",
      "Training Loss per 100 steps: 0.152394637465477\n",
      "Training Accuracy per 100 steps: 94.37767729652546\n",
      "Training Loss per 100 steps: 0.15178769826889038\n",
      "Training Accuracy per 100 steps: 94.44002726033621\n",
      "Training Loss per 100 steps: 0.1519700586795807\n",
      "Training Accuracy per 100 steps: 94.45349847892221\n",
      "Training Loss per 100 steps: 0.1526440531015396\n",
      "Training Accuracy per 100 steps: 94.41899208663057\n",
      "Training Loss per 100 steps: 0.15213435888290405\n",
      "Training Accuracy per 100 steps: 94.43222710915634\n",
      "Training Loss per 100 steps: 0.15220089256763458\n",
      "Training Accuracy per 100 steps: 94.43963860053826\n",
      "Training Loss per 100 steps: 0.15267831087112427\n",
      "Training Accuracy per 100 steps: 94.404850055535\n",
      "Training Loss per 100 steps: 0.1522454470396042\n",
      "Training Accuracy per 100 steps: 94.37700821135309\n",
      "Training Loss per 100 steps: 0.15005935728549957\n",
      "Training Accuracy per 100 steps: 94.49327817993795\n",
      "Training Loss per 100 steps: 0.1492127627134323\n",
      "Training Accuracy per 100 steps: 94.52682439186938\n",
      "Training Loss per 100 steps: 0.1485891491174698\n",
      "Training Accuracy per 100 steps: 94.56626894550145\n",
      "Training Loss per 100 steps: 0.1490827053785324\n",
      "Training Accuracy per 100 steps: 94.5446735395189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.14903971552848816\n",
      "Training Accuracy per 100 steps: 94.52438654953045\n",
      "Training Loss per 100 steps: 0.1490604430437088\n",
      "Training Accuracy per 100 steps: 94.51631872978535\n",
      "Training Loss per 100 steps: 0.14823706448078156\n",
      "Training Accuracy per 100 steps: 94.5444158811768\n",
      "Training Loss per 100 steps: 0.14697563648223877\n",
      "Training Accuracy per 100 steps: 94.58830880311025\n",
      "Training Loss per 100 steps: 0.14613106846809387\n",
      "Training Accuracy per 100 steps: 94.60956498243718\n",
      "Training Loss per 100 steps: 0.14585165679454803\n",
      "Training Accuracy per 100 steps: 94.62641410155223\n",
      "Training Loss per 100 steps: 0.14550620317459106\n",
      "Training Accuracy per 100 steps: 94.64560369136119\n",
      "Training Loss per 100 steps: 0.14547213912010193\n",
      "Training Accuracy per 100 steps: 94.63884028992751\n",
      "Training Loss per 100 steps: 0.14577199518680573\n",
      "Training Accuracy per 100 steps: 94.62326261887344\n",
      "Training Loss per 100 steps: 0.14557711780071259\n",
      "Training Accuracy per 100 steps: 94.63223042132826\n",
      "Training Loss per 100 steps: 0.1451472043991089\n",
      "Training Accuracy per 100 steps: 94.6524064171123\n",
      "Training Loss per 100 steps: 0.14579445123672485\n",
      "Training Accuracy per 100 steps: 94.6319018404908\n",
      "Training Loss per 100 steps: 0.14538618922233582\n",
      "Training Accuracy per 100 steps: 94.64563430348811\n",
      "Training Loss per 100 steps: 0.14528441429138184\n",
      "Training Accuracy per 100 steps: 94.65605303194958\n",
      "Training Loss per 100 steps: 0.14416003227233887\n",
      "Training Accuracy per 100 steps: 94.71123165283983\n",
      "Training Loss per 100 steps: 0.1439497023820877\n",
      "Training Accuracy per 100 steps: 94.72766090397833\n",
      "Training Loss per 100 steps: 0.14403820037841797\n",
      "Training Accuracy per 100 steps: 94.7051622117935\n",
      "The Total Accuracy for Epoch 1: 94.745\n",
      "Training Loss Epoch: 0.14328330755233765\n",
      "Training Accuracy Epoch: 94.745\n",
      "Validation Loss Epoch: 0.2606198787689209\n",
      "Validation Accuracy Epoch: 91.7\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../../models/best-ft-bert-cased-imdb-sentiment-maxlen256-bs8.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/models/best-ft-bert-cased-imdb-sentiment-maxlen256-bs8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    # tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    tokens = torch.tensor([tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2446,   92],\n",
       "       [ 316, 2146]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.84; Precision:95.88918677390528; Recall:87.16490658001625; F1-Score:91.31914893617021\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92      2538\n",
      "    positive       0.96      0.87      0.91      2462\n",
      "\n",
      "    accuracy                           0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.92      0.92      0.92      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
