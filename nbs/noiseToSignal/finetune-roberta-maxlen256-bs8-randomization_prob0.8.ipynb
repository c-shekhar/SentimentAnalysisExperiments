{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_dict = {\"negative\" : 0, \"positive\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    return p.clean(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/rand_prob_0.8.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_review</th>\n",
       "      <th>random_prob_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  target  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive       1   \n",
       "1  A wonderful little production. <br /><br />The...  positive       1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive       1   \n",
       "3  Basically there's a family where a little boy ...  negative       0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1   \n",
       "\n",
       "                                        clean_review  random_prob_target  \n",
       "0  One of the other reviewers has mentioned that ...                   0  \n",
       "1  A wonderful little production. <br /><br />The...                   1  \n",
       "2  I thought this was a wonderful way to spend ti...                   0  \n",
       "3  Basically there's a family where a little boy ...                   0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate for the randomized dataset is: 79.682\n"
     ]
    }
   ],
   "source": [
    "n_correct_labels_by_chance = len(df[df.target==df.random_prob_target])\n",
    "n_samples = len(df)\n",
    "correct_rate = (n_correct_labels_by_chance/n_samples)*100\n",
    "print(f\"Correct rate for the randomized dataset is: {correct_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.random_prob_target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 5)\n",
      "TRAIN Dataset: (40000, 5)\n",
      "VALID Dataset: (5000, 5)\n",
      "TEST Dataset: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6946576237678528\n",
      "Training Accuracy per 100 steps: 51.60891089108911\n",
      "Training Loss per 100 steps: 0.6431633830070496\n",
      "Training Accuracy per 100 steps: 62.18905472636816\n",
      "Training Loss per 100 steps: 0.6332181096076965\n",
      "Training Accuracy per 100 steps: 64.95016611295681\n",
      "Training Loss per 100 steps: 0.628638744354248\n",
      "Training Accuracy per 100 steps: 66.24064837905237\n",
      "Training Loss per 100 steps: 0.6141040325164795\n",
      "Training Accuracy per 100 steps: 68.13872255489022\n",
      "Training Loss per 100 steps: 0.6075103878974915\n",
      "Training Accuracy per 100 steps: 69.11397670549084\n",
      "Training Loss per 100 steps: 0.6039933562278748\n",
      "Training Accuracy per 100 steps: 69.56134094151213\n",
      "Training Loss per 100 steps: 0.598203182220459\n",
      "Training Accuracy per 100 steps: 70.20911360799002\n",
      "Training Loss per 100 steps: 0.5945924520492554\n",
      "Training Accuracy per 100 steps: 70.64372918978913\n",
      "Training Loss per 100 steps: 0.5942540168762207\n",
      "Training Accuracy per 100 steps: 70.74175824175825\n",
      "Training Loss per 100 steps: 0.5920273661613464\n",
      "Training Accuracy per 100 steps: 70.92415985467757\n",
      "Training Loss per 100 steps: 0.5906936526298523\n",
      "Training Accuracy per 100 steps: 71.14904246461282\n",
      "Training Loss per 100 steps: 0.5877656936645508\n",
      "Training Accuracy per 100 steps: 71.40661029976941\n",
      "Training Loss per 100 steps: 0.5846273899078369\n",
      "Training Accuracy per 100 steps: 71.70770877944325\n",
      "Training Loss per 100 steps: 0.5843589901924133\n",
      "Training Accuracy per 100 steps: 71.81878747501665\n",
      "Training Loss per 100 steps: 0.5833784937858582\n",
      "Training Accuracy per 100 steps: 71.98625858838226\n",
      "Training Loss per 100 steps: 0.5820531845092773\n",
      "Training Accuracy per 100 steps: 72.14873603762493\n",
      "Training Loss per 100 steps: 0.5806227326393127\n",
      "Training Accuracy per 100 steps: 72.3070516379789\n",
      "Training Loss per 100 steps: 0.5788145661354065\n",
      "Training Accuracy per 100 steps: 72.54076801683324\n",
      "Training Loss per 100 steps: 0.5786212682723999\n",
      "Training Accuracy per 100 steps: 72.54497751124438\n",
      "Training Loss per 100 steps: 0.5770952105522156\n",
      "Training Accuracy per 100 steps: 72.75107091861018\n",
      "Training Loss per 100 steps: 0.576892077922821\n",
      "Training Accuracy per 100 steps: 72.80213539300318\n",
      "Training Loss per 100 steps: 0.5761561393737793\n",
      "Training Accuracy per 100 steps: 72.83246414602347\n",
      "Training Loss per 100 steps: 0.5767002701759338\n",
      "Training Accuracy per 100 steps: 72.75093710953769\n",
      "Training Loss per 100 steps: 0.5771608352661133\n",
      "Training Accuracy per 100 steps: 72.70091963214715\n",
      "Training Loss per 100 steps: 0.5778880715370178\n",
      "Training Accuracy per 100 steps: 72.64513648596693\n",
      "Training Loss per 100 steps: 0.5765050053596497\n",
      "Training Accuracy per 100 steps: 72.80174009626064\n",
      "Training Loss per 100 steps: 0.5772083401679993\n",
      "Training Accuracy per 100 steps: 72.7820421278115\n",
      "Training Loss per 100 steps: 0.5761455297470093\n",
      "Training Accuracy per 100 steps: 72.85849706997587\n",
      "Training Loss per 100 steps: 0.5744061470031738\n",
      "Training Accuracy per 100 steps: 73.01316227924025\n",
      "Training Loss per 100 steps: 0.5747137665748596\n",
      "Training Accuracy per 100 steps: 73.01273782650757\n",
      "Training Loss per 100 steps: 0.5746001601219177\n",
      "Training Accuracy per 100 steps: 73.02405498281787\n",
      "Training Loss per 100 steps: 0.5739225745201111\n",
      "Training Accuracy per 100 steps: 73.0990608906392\n",
      "Training Loss per 100 steps: 0.5734385848045349\n",
      "Training Accuracy per 100 steps: 73.15495442516907\n",
      "Training Loss per 100 steps: 0.5731967091560364\n",
      "Training Accuracy per 100 steps: 73.1898029134533\n",
      "Training Loss per 100 steps: 0.5731973648071289\n",
      "Training Accuracy per 100 steps: 73.20188836434323\n",
      "Training Loss per 100 steps: 0.5730620622634888\n",
      "Training Accuracy per 100 steps: 73.24034044852742\n",
      "Training Loss per 100 steps: 0.5726513862609863\n",
      "Training Accuracy per 100 steps: 73.2603262299395\n",
      "Training Loss per 100 steps: 0.5722253918647766\n",
      "Training Accuracy per 100 steps: 73.32735196103563\n",
      "Training Loss per 100 steps: 0.5718840956687927\n",
      "Training Accuracy per 100 steps: 73.3816545863534\n",
      "Training Loss per 100 steps: 0.5711930990219116\n",
      "Training Accuracy per 100 steps: 73.43940502316508\n",
      "Training Loss per 100 steps: 0.571277379989624\n",
      "Training Accuracy per 100 steps: 73.45274934539395\n",
      "Training Loss per 100 steps: 0.5707972645759583\n",
      "Training Accuracy per 100 steps: 73.4945361543827\n",
      "Training Loss per 100 steps: 0.5712404251098633\n",
      "Training Accuracy per 100 steps: 73.45489661440581\n",
      "Training Loss per 100 steps: 0.5710629224777222\n",
      "Training Accuracy per 100 steps: 73.48644745612086\n",
      "Training Loss per 100 steps: 0.5704402923583984\n",
      "Training Accuracy per 100 steps: 73.51391001956097\n",
      "Training Loss per 100 steps: 0.5698646306991577\n",
      "Training Accuracy per 100 steps: 73.54552222931291\n",
      "Training Loss per 100 steps: 0.5700319409370422\n",
      "Training Accuracy per 100 steps: 73.52895230160384\n",
      "Training Loss per 100 steps: 0.569403886795044\n",
      "Training Accuracy per 100 steps: 73.56916955723322\n",
      "The Total Accuracy for Epoch 0: 73.605\n",
      "Training Loss Epoch: 0.5692221522331238\n",
      "Training Accuracy Epoch: 73.605\n",
      "Validation Loss Epoch: 0.553257405757904\n",
      "Validation Accuracy Epoch: 75.66\n",
      "Training Loss per 100 steps: 0.577236533164978\n",
      "Training Accuracy per 100 steps: 74.25742574257426\n",
      "Training Loss per 100 steps: 0.5642340779304504\n",
      "Training Accuracy per 100 steps: 75.24875621890547\n",
      "Training Loss per 100 steps: 0.5593711137771606\n",
      "Training Accuracy per 100 steps: 75.29069767441861\n",
      "Training Loss per 100 steps: 0.5688337683677673\n",
      "Training Accuracy per 100 steps: 74.34538653366583\n",
      "Training Loss per 100 steps: 0.5551248788833618\n",
      "Training Accuracy per 100 steps: 75.39920159680639\n",
      "Training Loss per 100 steps: 0.5537019968032837\n",
      "Training Accuracy per 100 steps: 75.43677204658901\n",
      "Training Loss per 100 steps: 0.5527811646461487\n",
      "Training Accuracy per 100 steps: 75.42796005706134\n",
      "Training Loss per 100 steps: 0.5490320324897766\n",
      "Training Accuracy per 100 steps: 75.59300873907615\n",
      "Training Loss per 100 steps: 0.546845555305481\n",
      "Training Accuracy per 100 steps: 75.72142064372919\n",
      "Training Loss per 100 steps: 0.5475094318389893\n",
      "Training Accuracy per 100 steps: 75.66183816183816\n",
      "Training Loss per 100 steps: 0.5454117059707642\n",
      "Training Accuracy per 100 steps: 75.84014532243415\n",
      "Training Loss per 100 steps: 0.545077383518219\n",
      "Training Accuracy per 100 steps: 75.84304746044963\n",
      "Training Loss per 100 steps: 0.5432700514793396\n",
      "Training Accuracy per 100 steps: 75.92236740968485\n",
      "Training Loss per 100 steps: 0.5415050983428955\n",
      "Training Accuracy per 100 steps: 75.99928622412563\n",
      "Training Loss per 100 steps: 0.5420404672622681\n",
      "Training Accuracy per 100 steps: 76.03264490339774\n",
      "Training Loss per 100 steps: 0.5407808423042297\n",
      "Training Accuracy per 100 steps: 76.13210493441599\n",
      "Training Loss per 100 steps: 0.5399243831634521\n",
      "Training Accuracy per 100 steps: 76.22721928277484\n",
      "Training Loss per 100 steps: 0.5386729836463928\n",
      "Training Accuracy per 100 steps: 76.29094947251527\n",
      "Training Loss per 100 steps: 0.5381091833114624\n",
      "Training Accuracy per 100 steps: 76.37427669647553\n",
      "Training Loss per 100 steps: 0.5380010008811951\n",
      "Training Accuracy per 100 steps: 76.34307846076962\n",
      "Training Loss per 100 steps: 0.5367783904075623\n",
      "Training Accuracy per 100 steps: 76.47548786292242\n",
      "Training Loss per 100 steps: 0.5371297001838684\n",
      "Training Accuracy per 100 steps: 76.49363925488414\n",
      "Training Loss per 100 steps: 0.536885678768158\n",
      "Training Accuracy per 100 steps: 76.48848326814428\n",
      "Training Loss per 100 steps: 0.5384438037872314\n",
      "Training Accuracy per 100 steps: 76.31715951686797\n",
      "Training Loss per 100 steps: 0.5388778448104858\n",
      "Training Accuracy per 100 steps: 76.32447021191524\n",
      "Training Loss per 100 steps: 0.5399786829948425\n",
      "Training Accuracy per 100 steps: 76.22068435217224\n",
      "Training Loss per 100 steps: 0.5389904379844666\n",
      "Training Accuracy per 100 steps: 76.31895594224362\n",
      "Training Loss per 100 steps: 0.5404502749443054\n",
      "Training Accuracy per 100 steps: 76.24062834701893\n",
      "Training Loss per 100 steps: 0.53959721326828\n",
      "Training Accuracy per 100 steps: 76.30558428128232\n",
      "Training Loss per 100 steps: 0.5385977625846863\n",
      "Training Accuracy per 100 steps: 76.38287237587471\n",
      "Training Loss per 100 steps: 0.5393616557121277\n",
      "Training Accuracy per 100 steps: 76.32618510158014\n",
      "Training Loss per 100 steps: 0.539769172668457\n",
      "Training Accuracy per 100 steps: 76.3120899718838\n",
      "Training Loss per 100 steps: 0.5391671061515808\n",
      "Training Accuracy per 100 steps: 76.37079672826417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.5383278727531433\n",
      "Training Accuracy per 100 steps: 76.4481034989709\n",
      "Training Loss per 100 steps: 0.5385538339614868\n",
      "Training Accuracy per 100 steps: 76.41388174807199\n",
      "Training Loss per 100 steps: 0.539456307888031\n",
      "Training Accuracy per 100 steps: 76.32949180783116\n",
      "Training Loss per 100 steps: 0.5393965840339661\n",
      "Training Accuracy per 100 steps: 76.33072142664145\n",
      "Training Loss per 100 steps: 0.5393562912940979\n",
      "Training Accuracy per 100 steps: 76.30557747961063\n",
      "Training Loss per 100 steps: 0.5392147898674011\n",
      "Training Accuracy per 100 steps: 76.33619584721866\n",
      "Training Loss per 100 steps: 0.5389130711555481\n",
      "Training Accuracy per 100 steps: 76.37778055486129\n",
      "Training Loss per 100 steps: 0.5386479496955872\n",
      "Training Accuracy per 100 steps: 76.41124116069251\n",
      "Training Loss per 100 steps: 0.5390101671218872\n",
      "Training Accuracy per 100 steps: 76.38657462508927\n",
      "Training Loss per 100 steps: 0.5387704968452454\n",
      "Training Accuracy per 100 steps: 76.40955591722854\n",
      "Training Loss per 100 steps: 0.5393363237380981\n",
      "Training Accuracy per 100 steps: 76.35764598954783\n",
      "Training Loss per 100 steps: 0.5393517017364502\n",
      "Training Accuracy per 100 steps: 76.35247722728283\n",
      "Training Loss per 100 steps: 0.5385338664054871\n",
      "Training Accuracy per 100 steps: 76.4073027602695\n",
      "Training Loss per 100 steps: 0.5378686189651489\n",
      "Training Accuracy per 100 steps: 76.46511380557328\n",
      "Training Loss per 100 steps: 0.5380809903144836\n",
      "Training Accuracy per 100 steps: 76.46844407415122\n",
      "Training Loss per 100 steps: 0.5376012325286865\n",
      "Training Accuracy per 100 steps: 76.51244643950214\n",
      "The Total Accuracy for Epoch 1: 76.545\n",
      "Training Loss Epoch: 0.537189245223999\n",
      "Training Accuracy Epoch: 76.545\n",
      "Validation Loss Epoch: 0.5627005100250244\n",
      "Validation Accuracy Epoch: 75.04\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.8.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized.pt'\n",
    "torch.save(model, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Epoch vs Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/nts/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on probabilitiscally randomized test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1949,  619],\n",
       "       [ 572, 1860]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.random_prob_target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.18; Precision:75.03025413473175; Recall:76.48026315789474; F1-Score:75.74832009773976\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.76      0.77      2568\n",
      "    positive       0.75      0.76      0.76      2432\n",
      "\n",
      "    accuracy                           0.76      5000\n",
      "   macro avg       0.76      0.76      0.76      5000\n",
      "weighted avg       0.76      0.76      0.76      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on real test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2383,  155],\n",
       "       [ 138, 2324]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.14; Precision:93.74747882210569; Recall:94.39480097481722; F1-Score:94.07002631046348\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.94      2538\n",
      "    positive       0.94      0.94      0.94      2462\n",
      "\n",
      "    accuracy                           0.94      5000\n",
      "   macro avg       0.94      0.94      0.94      5000\n",
      "weighted avg       0.94      0.94      0.94      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
