{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_dict = {\"negative\" : 0, \"positive\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    return p.clean(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/rand_prob_0.6.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_review</th>\n",
       "      <th>random_prob_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  target  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive       1   \n",
       "1  A wonderful little production. <br /><br />The...  positive       1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive       1   \n",
       "3  Basically there's a family where a little boy ...  negative       0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1   \n",
       "\n",
       "                                        clean_review  random_prob_target  \n",
       "0  One of the other reviewers has mentioned that ...                   0  \n",
       "1  A wonderful little production. <br /><br />The...                   1  \n",
       "2  I thought this was a wonderful way to spend ti...                   0  \n",
       "3  Basically there's a family where a little boy ...                   0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate for the randomized dataset is: 59.785999999999994\n"
     ]
    }
   ],
   "source": [
    "n_correct_labels_by_chance = len(df[df.target==df.random_prob_target])\n",
    "n_samples = len(df)\n",
    "correct_rate = (n_correct_labels_by_chance/n_samples)*100\n",
    "print(f\"Correct rate for the randomized dataset is: {correct_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.random_prob_target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 5)\n",
      "TRAIN Dataset: (40000, 5)\n",
      "VALID Dataset: (5000, 5)\n",
      "TEST Dataset: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.7002657651901245\n",
      "Training Accuracy per 100 steps: 47.648514851485146\n",
      "Training Loss per 100 steps: 0.6974096298217773\n",
      "Training Accuracy per 100 steps: 49.440298507462686\n",
      "Training Loss per 100 steps: 0.6962448358535767\n",
      "Training Accuracy per 100 steps: 50.20764119601329\n",
      "Training Loss per 100 steps: 0.6955511569976807\n",
      "Training Accuracy per 100 steps: 50.249376558603494\n",
      "Training Loss per 100 steps: 0.6957535743713379\n",
      "Training Accuracy per 100 steps: 50.199600798403196\n",
      "Training Loss per 100 steps: 0.6949617266654968\n",
      "Training Accuracy per 100 steps: 50.707154742096506\n",
      "Training Loss per 100 steps: 0.6947634816169739\n",
      "Training Accuracy per 100 steps: 51.12339514978602\n",
      "Training Loss per 100 steps: 0.6948354244232178\n",
      "Training Accuracy per 100 steps: 50.842696629213485\n",
      "Training Loss per 100 steps: 0.6947222948074341\n",
      "Training Accuracy per 100 steps: 50.86015538290788\n",
      "Training Loss per 100 steps: 0.6946422457695007\n",
      "Training Accuracy per 100 steps: 50.886613386613384\n",
      "Training Loss per 100 steps: 0.6943786144256592\n",
      "Training Accuracy per 100 steps: 51.23751135331517\n",
      "Training Loss per 100 steps: 0.6938960552215576\n",
      "Training Accuracy per 100 steps: 51.38426311407161\n",
      "Training Loss per 100 steps: 0.6931942701339722\n",
      "Training Accuracy per 100 steps: 51.82551883166795\n",
      "Training Loss per 100 steps: 0.6926659941673279\n",
      "Training Accuracy per 100 steps: 52.13240542469664\n",
      "Training Loss per 100 steps: 0.6924173831939697\n",
      "Training Accuracy per 100 steps: 52.481678880746166\n",
      "Training Loss per 100 steps: 0.6919834613800049\n",
      "Training Accuracy per 100 steps: 52.68582136164897\n",
      "Training Loss per 100 steps: 0.6924272775650024\n",
      "Training Accuracy per 100 steps: 52.46913580246913\n",
      "Training Loss per 100 steps: 0.6924050450325012\n",
      "Training Accuracy per 100 steps: 52.66518600777346\n",
      "Training Loss per 100 steps: 0.6919814944267273\n",
      "Training Accuracy per 100 steps: 52.82745923198317\n",
      "Training Loss per 100 steps: 0.6911178827285767\n",
      "Training Accuracy per 100 steps: 53.22963518240879\n",
      "Training Loss per 100 steps: 0.690389096736908\n",
      "Training Accuracy per 100 steps: 53.54593050928129\n",
      "Training Loss per 100 steps: 0.6903941631317139\n",
      "Training Accuracy per 100 steps: 53.62335302135393\n",
      "Training Loss per 100 steps: 0.6905115246772766\n",
      "Training Accuracy per 100 steps: 53.68861364624077\n",
      "Training Loss per 100 steps: 0.690406084060669\n",
      "Training Accuracy per 100 steps: 53.79008746355685\n",
      "Training Loss per 100 steps: 0.6903035640716553\n",
      "Training Accuracy per 100 steps: 53.8234706117553\n",
      "Training Loss per 100 steps: 0.6903124451637268\n",
      "Training Accuracy per 100 steps: 53.82545174932718\n",
      "Training Loss per 100 steps: 0.6900590658187866\n",
      "Training Accuracy per 100 steps: 53.98926323583858\n",
      "Training Loss per 100 steps: 0.689988374710083\n",
      "Training Accuracy per 100 steps: 54.06104962513388\n",
      "Training Loss per 100 steps: 0.6896069645881653\n",
      "Training Accuracy per 100 steps: 54.248534987935194\n",
      "Training Loss per 100 steps: 0.6894416809082031\n",
      "Training Accuracy per 100 steps: 54.39853382205931\n",
      "Training Loss per 100 steps: 0.6896051168441772\n",
      "Training Accuracy per 100 steps: 54.35343437600774\n",
      "Training Loss per 100 steps: 0.6894313097000122\n",
      "Training Accuracy per 100 steps: 54.40877850671665\n",
      "Training Loss per 100 steps: 0.6891354918479919\n",
      "Training Accuracy per 100 steps: 54.52514389578916\n",
      "Training Loss per 100 steps: 0.6887787580490112\n",
      "Training Accuracy per 100 steps: 54.63099088503381\n",
      "Training Loss per 100 steps: 0.6885108947753906\n",
      "Training Accuracy per 100 steps: 54.730791202513565\n",
      "Training Loss per 100 steps: 0.6885616183280945\n",
      "Training Accuracy per 100 steps: 54.73479589003055\n",
      "Training Loss per 100 steps: 0.6885514855384827\n",
      "Training Accuracy per 100 steps: 54.728451769791945\n",
      "Training Loss per 100 steps: 0.6885279417037964\n",
      "Training Accuracy per 100 steps: 54.76519337016575\n",
      "Training Loss per 100 steps: 0.6884192228317261\n",
      "Training Accuracy per 100 steps: 54.84811586772622\n",
      "Training Loss per 100 steps: 0.6882604360580444\n",
      "Training Accuracy per 100 steps: 54.93626593351662\n",
      "Training Loss per 100 steps: 0.6880314350128174\n",
      "Training Accuracy per 100 steps: 55.00487685930261\n",
      "Training Loss per 100 steps: 0.687969982624054\n",
      "Training Accuracy per 100 steps: 55.02856462746965\n",
      "Training Loss per 100 steps: 0.6876534819602966\n",
      "Training Accuracy per 100 steps: 55.16159032783074\n",
      "Training Loss per 100 steps: 0.687690019607544\n",
      "Training Accuracy per 100 steps: 55.17496023630993\n",
      "Training Loss per 100 steps: 0.687732994556427\n",
      "Training Accuracy per 100 steps: 55.17662741612975\n",
      "Training Loss per 100 steps: 0.6875993013381958\n",
      "Training Accuracy per 100 steps: 55.229841338839385\n",
      "Training Loss per 100 steps: 0.6874847412109375\n",
      "Training Accuracy per 100 steps: 55.256860242501595\n",
      "Training Loss per 100 steps: 0.6875179409980774\n",
      "Training Accuracy per 100 steps: 55.24369922932723\n",
      "Training Loss per 100 steps: 0.6873263120651245\n",
      "Training Accuracy per 100 steps: 55.31524178739033\n",
      "The Total Accuracy for Epoch 0: 55.3375\n",
      "Training Loss Epoch: 0.6872700452804565\n",
      "Training Accuracy Epoch: 55.3375\n",
      "Validation Loss Epoch: 0.680546760559082\n",
      "Validation Accuracy Epoch: 57.8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 335649408 vs 335649296",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-25c37711695a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.6.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 335649408 vs 335649296"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.6.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.6.pt'\n",
    "torch.save(model, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Epoch vs Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/nts/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on probabilitiscally randomized test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1207, 1317],\n",
       "       [ 860, 1616]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.random_prob_target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.46; Precision:55.09717013296965; Recall:65.26655896607431; F1-Score:59.75226474394527\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.48      0.53      2524\n",
      "    positive       0.55      0.65      0.60      2476\n",
      "\n",
      "    accuracy                           0.56      5000\n",
      "   macro avg       0.57      0.57      0.56      5000\n",
      "weighted avg       0.57      0.56      0.56      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on real test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1987,  551],\n",
       "       [  80, 2382]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.38; Precision:81.21377429253324; Recall:96.75060926076361; F1-Score:88.30398517145504\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.78      0.86      2538\n",
      "    positive       0.81      0.97      0.88      2462\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.89      0.88      0.87      5000\n",
      "weighted avg       0.89      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
