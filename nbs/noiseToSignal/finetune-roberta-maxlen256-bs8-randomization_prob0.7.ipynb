{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_dict = {\"negative\" : 0, \"positive\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    return p.clean(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/rand_prob_0.7.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_review</th>\n",
       "      <th>random_prob_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  target  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive       1   \n",
       "1  A wonderful little production. <br /><br />The...  positive       1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive       1   \n",
       "3  Basically there's a family where a little boy ...  negative       0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1   \n",
       "\n",
       "                                        clean_review  random_prob_target  \n",
       "0  One of the other reviewers has mentioned that ...                   0  \n",
       "1  A wonderful little production. <br /><br />The...                   1  \n",
       "2  I thought this was a wonderful way to spend ti...                   0  \n",
       "3  Basically there's a family where a little boy ...                   0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate for the randomized dataset is: 69.89999999999999\n"
     ]
    }
   ],
   "source": [
    "n_correct_labels_by_chance = len(df[df.target==df.random_prob_target])\n",
    "n_samples = len(df)\n",
    "correct_rate = (n_correct_labels_by_chance/n_samples)*100\n",
    "print(f\"Correct rate for the randomized dataset is: {correct_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.random_prob_target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 5)\n",
      "TRAIN Dataset: (40000, 5)\n",
      "VALID Dataset: (5000, 5)\n",
      "TEST Dataset: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6984294652938843\n",
      "Training Accuracy per 100 steps: 50.0\n",
      "Training Loss per 100 steps: 0.6936253905296326\n",
      "Training Accuracy per 100 steps: 51.92786069651741\n",
      "Training Loss per 100 steps: 0.6833276748657227\n",
      "Training Accuracy per 100 steps: 54.90033222591362\n",
      "Training Loss per 100 steps: 0.682627260684967\n",
      "Training Accuracy per 100 steps: 55.29925187032419\n",
      "Training Loss per 100 steps: 0.6766214966773987\n",
      "Training Accuracy per 100 steps: 57.010978043912175\n",
      "Training Loss per 100 steps: 0.6724893450737\n",
      "Training Accuracy per 100 steps: 58.21547420965058\n",
      "Training Loss per 100 steps: 0.66948401927948\n",
      "Training Accuracy per 100 steps: 58.88017118402283\n",
      "Training Loss per 100 steps: 0.6656408905982971\n",
      "Training Accuracy per 100 steps: 59.78464419475655\n",
      "Training Loss per 100 steps: 0.6639408469200134\n",
      "Training Accuracy per 100 steps: 60.405105438401776\n",
      "Training Loss per 100 steps: 0.6641787886619568\n",
      "Training Accuracy per 100 steps: 60.464535464535466\n",
      "Training Loss per 100 steps: 0.6621655821800232\n",
      "Training Accuracy per 100 steps: 60.967302452316076\n",
      "Training Loss per 100 steps: 0.6598475575447083\n",
      "Training Accuracy per 100 steps: 61.46960865945046\n",
      "Training Loss per 100 steps: 0.6586568355560303\n",
      "Training Accuracy per 100 steps: 61.71214450422752\n",
      "Training Loss per 100 steps: 0.6562479138374329\n",
      "Training Accuracy per 100 steps: 62.232334047109205\n",
      "Training Loss per 100 steps: 0.6546615362167358\n",
      "Training Accuracy per 100 steps: 62.616588940706194\n",
      "Training Loss per 100 steps: 0.6533325910568237\n",
      "Training Accuracy per 100 steps: 62.921611492816986\n",
      "Training Loss per 100 steps: 0.6531456112861633\n",
      "Training Accuracy per 100 steps: 63.080540858318635\n",
      "Training Loss per 100 steps: 0.6529104709625244\n",
      "Training Accuracy per 100 steps: 63.180177679067185\n",
      "Training Loss per 100 steps: 0.6520087122917175\n",
      "Training Accuracy per 100 steps: 63.3022093634929\n",
      "Training Loss per 100 steps: 0.6508440971374512\n",
      "Training Accuracy per 100 steps: 63.52448775612194\n",
      "Training Loss per 100 steps: 0.6502864956855774\n",
      "Training Accuracy per 100 steps: 63.61851499286054\n",
      "Training Loss per 100 steps: 0.6503350734710693\n",
      "Training Accuracy per 100 steps: 63.66424352567015\n",
      "Training Loss per 100 steps: 0.6499147415161133\n",
      "Training Accuracy per 100 steps: 63.749456757931334\n",
      "Training Loss per 100 steps: 0.6503353714942932\n",
      "Training Accuracy per 100 steps: 63.70262390670554\n",
      "Training Loss per 100 steps: 0.6508532166481018\n",
      "Training Accuracy per 100 steps: 63.67453018792483\n",
      "Training Loss per 100 steps: 0.6515211462974548\n",
      "Training Accuracy per 100 steps: 63.57650903498654\n",
      "Training Loss per 100 steps: 0.6502211689949036\n",
      "Training Accuracy per 100 steps: 63.80970011106997\n",
      "Training Loss per 100 steps: 0.6499806046485901\n",
      "Training Accuracy per 100 steps: 63.88343448768297\n",
      "Training Loss per 100 steps: 0.6490352749824524\n",
      "Training Accuracy per 100 steps: 64.0382626680455\n",
      "Training Loss per 100 steps: 0.6481888890266418\n",
      "Training Accuracy per 100 steps: 64.21609463512162\n",
      "Training Loss per 100 steps: 0.6489230990409851\n",
      "Training Accuracy per 100 steps: 64.12447597549178\n",
      "Training Loss per 100 steps: 0.6486320495605469\n",
      "Training Accuracy per 100 steps: 64.18697282099345\n",
      "Training Loss per 100 steps: 0.6484283208847046\n",
      "Training Accuracy per 100 steps: 64.19266888821569\n",
      "Training Loss per 100 steps: 0.6479256749153137\n",
      "Training Accuracy per 100 steps: 64.30094089973537\n",
      "Training Loss per 100 steps: 0.6474795341491699\n",
      "Training Accuracy per 100 steps: 64.37089403027706\n",
      "Training Loss per 100 steps: 0.64764004945755\n",
      "Training Accuracy per 100 steps: 64.38489308525409\n",
      "Training Loss per 100 steps: 0.6473914384841919\n",
      "Training Accuracy per 100 steps: 64.43191029451499\n",
      "Training Loss per 100 steps: 0.6473142504692078\n",
      "Training Accuracy per 100 steps: 64.41725861615365\n",
      "Training Loss per 100 steps: 0.6472863554954529\n",
      "Training Accuracy per 100 steps: 64.44180979236093\n",
      "Training Loss per 100 steps: 0.6470772624015808\n",
      "Training Accuracy per 100 steps: 64.51512121969508\n",
      "Training Loss per 100 steps: 0.6468763947486877\n",
      "Training Accuracy per 100 steps: 64.53608875883931\n",
      "Training Loss per 100 steps: 0.6471065282821655\n",
      "Training Accuracy per 100 steps: 64.52035229707212\n",
      "Training Loss per 100 steps: 0.6467001438140869\n",
      "Training Accuracy per 100 steps: 64.58381771681005\n",
      "Training Loss per 100 steps: 0.6467583179473877\n",
      "Training Accuracy per 100 steps: 64.57623267439219\n",
      "Training Loss per 100 steps: 0.64687579870224\n",
      "Training Accuracy per 100 steps: 64.58564763385914\n",
      "Training Loss per 100 steps: 0.6466250419616699\n",
      "Training Accuracy per 100 steps: 64.61095414040426\n",
      "Training Loss per 100 steps: 0.6464654803276062\n",
      "Training Accuracy per 100 steps: 64.63784301212507\n",
      "Training Loss per 100 steps: 0.6464822292327881\n",
      "Training Accuracy per 100 steps: 64.6479900020829\n",
      "Training Loss per 100 steps: 0.6459630727767944\n",
      "Training Accuracy per 100 steps: 64.72148541114058\n",
      "The Total Accuracy for Epoch 0: 64.725\n",
      "Training Loss Epoch: 0.646070659160614\n",
      "Training Accuracy Epoch: 64.725\n",
      "Validation Loss Epoch: 0.6359872817993164\n",
      "Validation Accuracy Epoch: 66.12\n",
      "Training Loss per 100 steps: 0.6546729207038879\n",
      "Training Accuracy per 100 steps: 64.72772277227723\n",
      "Training Loss per 100 steps: 0.6452928781509399\n",
      "Training Accuracy per 100 steps: 65.98258706467662\n",
      "Training Loss per 100 steps: 0.634644627571106\n",
      "Training Accuracy per 100 steps: 66.86046511627907\n",
      "Training Loss per 100 steps: 0.641587495803833\n",
      "Training Accuracy per 100 steps: 66.14713216957605\n",
      "Training Loss per 100 steps: 0.6376720070838928\n",
      "Training Accuracy per 100 steps: 66.64171656686626\n",
      "Training Loss per 100 steps: 0.6360985636711121\n",
      "Training Accuracy per 100 steps: 66.80532445923461\n",
      "Training Loss per 100 steps: 0.6339136958122253\n",
      "Training Accuracy per 100 steps: 66.86875891583452\n",
      "Training Loss per 100 steps: 0.6311228275299072\n",
      "Training Accuracy per 100 steps: 67.38451935081149\n",
      "Training Loss per 100 steps: 0.6303670406341553\n",
      "Training Accuracy per 100 steps: 67.52219755826859\n",
      "Training Loss per 100 steps: 0.6312263011932373\n",
      "Training Accuracy per 100 steps: 67.34515484515485\n",
      "Training Loss per 100 steps: 0.6289528608322144\n",
      "Training Accuracy per 100 steps: 67.60899182561307\n",
      "Training Loss per 100 steps: 0.627208411693573\n",
      "Training Accuracy per 100 steps: 67.87052456286428\n",
      "Training Loss per 100 steps: 0.627109944820404\n",
      "Training Accuracy per 100 steps: 67.94773251345119\n",
      "Training Loss per 100 steps: 0.6258245706558228\n",
      "Training Accuracy per 100 steps: 68.14775160599572\n",
      "Training Loss per 100 steps: 0.6254987716674805\n",
      "Training Accuracy per 100 steps: 68.20453031312458\n",
      "Training Loss per 100 steps: 0.6250454187393188\n",
      "Training Accuracy per 100 steps: 68.30106183635228\n",
      "Training Loss per 100 steps: 0.6252913475036621\n",
      "Training Accuracy per 100 steps: 68.2392710170488\n",
      "Training Loss per 100 steps: 0.6250556111335754\n",
      "Training Accuracy per 100 steps: 68.27456968350916\n",
      "Training Loss per 100 steps: 0.6249809265136719\n",
      "Training Accuracy per 100 steps: 68.29300368227248\n",
      "Training Loss per 100 steps: 0.6241447329521179\n",
      "Training Accuracy per 100 steps: 68.41579210394802\n",
      "Training Loss per 100 steps: 0.623706042766571\n",
      "Training Accuracy per 100 steps: 68.46144693003332\n",
      "Training Loss per 100 steps: 0.6243155598640442\n",
      "Training Accuracy per 100 steps: 68.34393457519309\n",
      "Training Loss per 100 steps: 0.6241567730903625\n",
      "Training Accuracy per 100 steps: 68.38331160365058\n",
      "Training Loss per 100 steps: 0.6250914931297302\n",
      "Training Accuracy per 100 steps: 68.25801749271137\n",
      "Training Loss per 100 steps: 0.6259992718696594\n",
      "Training Accuracy per 100 steps: 68.13774490203919\n",
      "Training Loss per 100 steps: 0.6270409226417542\n",
      "Training Accuracy per 100 steps: 67.99788542868127\n",
      "Training Loss per 100 steps: 0.6260306239128113\n",
      "Training Accuracy per 100 steps: 68.10440577563865\n",
      "Training Loss per 100 steps: 0.6259839534759521\n",
      "Training Accuracy per 100 steps: 68.11406640485541\n",
      "Training Loss per 100 steps: 0.6250455975532532\n",
      "Training Accuracy per 100 steps: 68.23509134781109\n",
      "Training Loss per 100 steps: 0.6242125034332275\n",
      "Training Accuracy per 100 steps: 68.34388537154283\n",
      "Training Loss per 100 steps: 0.6253505349159241\n",
      "Training Accuracy per 100 steps: 68.19171235085456\n",
      "Training Loss per 100 steps: 0.6253869533538818\n",
      "Training Accuracy per 100 steps: 68.19743830053109\n",
      "Training Loss per 100 steps: 0.6250469088554382\n",
      "Training Accuracy per 100 steps: 68.22175098455014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6245233416557312\n",
      "Training Accuracy per 100 steps: 68.28138782710967\n",
      "Training Loss per 100 steps: 0.6241372227668762\n",
      "Training Accuracy per 100 steps: 68.31619537275064\n",
      "Training Loss per 100 steps: 0.6244093775749207\n",
      "Training Accuracy per 100 steps: 68.26228825326298\n",
      "Training Loss per 100 steps: 0.624466598033905\n",
      "Training Accuracy per 100 steps: 68.24506890029721\n",
      "Training Loss per 100 steps: 0.6245073676109314\n",
      "Training Accuracy per 100 steps: 68.24191002367797\n",
      "Training Loss per 100 steps: 0.6246717572212219\n",
      "Training Accuracy per 100 steps: 68.20046142014868\n",
      "Training Loss per 100 steps: 0.6246057748794556\n",
      "Training Accuracy per 100 steps: 68.19232691827044\n",
      "Training Loss per 100 steps: 0.624449610710144\n",
      "Training Accuracy per 100 steps: 68.2181175323092\n",
      "Training Loss per 100 steps: 0.6248944997787476\n",
      "Training Accuracy per 100 steps: 68.17424422756487\n",
      "Training Loss per 100 steps: 0.6246791481971741\n",
      "Training Accuracy per 100 steps: 68.20797488956056\n",
      "Training Loss per 100 steps: 0.6246901154518127\n",
      "Training Accuracy per 100 steps: 68.18904794364917\n",
      "Training Loss per 100 steps: 0.6248360872268677\n",
      "Training Accuracy per 100 steps: 68.17096200844257\n",
      "Training Loss per 100 steps: 0.6245354413986206\n",
      "Training Accuracy per 100 steps: 68.21886546402956\n",
      "Training Loss per 100 steps: 0.6245510578155518\n",
      "Training Accuracy per 100 steps: 68.22484577749415\n",
      "Training Loss per 100 steps: 0.6247938871383667\n",
      "Training Accuracy per 100 steps: 68.17069360549885\n",
      "Training Loss per 100 steps: 0.6244595646858215\n",
      "Training Accuracy per 100 steps: 68.21056927157723\n",
      "The Total Accuracy for Epoch 1: 68.17\n",
      "Training Loss Epoch: 0.6246369481086731\n",
      "Training Accuracy Epoch: 68.17\n",
      "Validation Loss Epoch: 0.6455210447311401\n",
      "Validation Accuracy Epoch: 65.92\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.7.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = f'../../models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized.pt'\n",
    "torch.save(model, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Epoch vs Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/lazylearner/Data/joni/models/best-ft-roberta-imdb-sentiment-maxlen256-bs8-randomized_0.7.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on probabilitiscally randomized test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1645,  926],\n",
       "       [ 748, 1681]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.random_prob_target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.52; Precision:64.48024549290372; Recall:69.20543433511733; F1-Score:66.75933280381255\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.64      0.66      2571\n",
      "    positive       0.64      0.69      0.67      2429\n",
      "\n",
      "    accuracy                           0.67      5000\n",
      "   macro avg       0.67      0.67      0.67      5000\n",
      "weighted avg       0.67      0.67      0.67      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on real test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2287,  251],\n",
       "       [ 106, 2356]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred,zero_division=1)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "f1 = f1_score(y_true,y_pred,zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.86; Precision:90.37207518220175; Recall:95.69455727051178; F1-Score:92.95719076740974\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}; F1-Score:{f1*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.90      0.93      2538\n",
      "    positive       0.90      0.96      0.93      2462\n",
      "\n",
      "    accuracy                           0.93      5000\n",
      "   macro avg       0.93      0.93      0.93      5000\n",
      "weighted avg       0.93      0.93      0.93      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"],zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
