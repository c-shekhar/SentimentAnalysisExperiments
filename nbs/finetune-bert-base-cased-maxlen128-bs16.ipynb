{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_dict = {\"negative\" : 0, \"positive\" : 1}\n",
    "def encode_label(x):\n",
    "    return encoded_label_dict.get(x,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../data/imdb/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"sentiment\"].apply(lambda x: encode_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    return p.clean(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_review\"] = df[\"review\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  target  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive       1   \n",
       "1  A wonderful little production. <br /><br />The...  positive       1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive       1   \n",
       "3  Basically there's a family where a little boy ...  negative       0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1   \n",
       "\n",
       "                                        clean_review  \n",
       "0  One of the other reviewers has mentioned that ...  \n",
       "1  A wonderful little production. <br /><br />The...  \n",
       "2  I thought this was a wonderful way to spend ti...  \n",
       "3  Basically there's a family where a little boy ...  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/imdb/imdb_preprocessed_reviews.csv\",index=None)\n",
    "\n",
    "fp = \"../data/imdb/imdb_preprocessed_reviews.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in df.clean_review]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.clean_review[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, stratify=None, random_state=2022)\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=None, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (50000, 4)\n",
      "TRAIN Dataset: (40000, 4)\n",
      "VALID Dataset: (5000, 4)\n",
      "TEST Dataset: (5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader\n",
    "train_dataset = train.reset_index(drop=True)\n",
    "valid_dataset = valid.reset_index(drop=True)\n",
    "test_dataset = test.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "validating_set = Triage(valid_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    state = torch.get_rng_state()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            tr_loss += loss\n",
    "            big_val, big_idx = torch.max(logits, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    torch.set_rng_state(state)\n",
    "    return epoch_loss, epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the roberta model\n",
    "def train(epoch, training_loader, testing_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        tr_loss += loss\n",
    "        big_val, big_idx = torch.max(logits, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _!=0 and _%100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    valid_loss, valid_accu = validate(model,testing_loader)\n",
    "    return model, epoch_loss, epoch_accu, valid_loss, valid_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lazylearner/anaconda3/envs/joni/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 100 steps: 0.6118298768997192\n",
      "Training Accuracy per 100 steps: 64.85148514851485\n",
      "Training Loss per 100 steps: 0.4989476501941681\n",
      "Training Accuracy per 100 steps: 74.28482587064677\n",
      "Training Loss per 100 steps: 0.45078977942466736\n",
      "Training Accuracy per 100 steps: 77.55398671096346\n",
      "Training Loss per 100 steps: 0.43292152881622314\n",
      "Training Accuracy per 100 steps: 78.88092269326684\n",
      "Training Loss per 100 steps: 0.4185851514339447\n",
      "Training Accuracy per 100 steps: 79.87774451097805\n",
      "Training Loss per 100 steps: 0.40553203225135803\n",
      "Training Accuracy per 100 steps: 80.76123128119801\n",
      "Training Loss per 100 steps: 0.3975709080696106\n",
      "Training Accuracy per 100 steps: 81.13409415121255\n",
      "Training Loss per 100 steps: 0.3879895508289337\n",
      "Training Accuracy per 100 steps: 81.74937578027466\n",
      "Training Loss per 100 steps: 0.3800363838672638\n",
      "Training Accuracy per 100 steps: 82.31825749167592\n",
      "Training Loss per 100 steps: 0.372728168964386\n",
      "Training Accuracy per 100 steps: 82.7485014985015\n",
      "Training Loss per 100 steps: 0.36671650409698486\n",
      "Training Accuracy per 100 steps: 83.19709355131698\n",
      "Training Loss per 100 steps: 0.3636487126350403\n",
      "Training Accuracy per 100 steps: 83.47210657785179\n",
      "Training Loss per 100 steps: 0.3595367968082428\n",
      "Training Accuracy per 100 steps: 83.73847040737894\n",
      "Training Loss per 100 steps: 0.3578135669231415\n",
      "Training Accuracy per 100 steps: 83.89097073518916\n",
      "Training Loss per 100 steps: 0.35294830799102783\n",
      "Training Accuracy per 100 steps: 84.14390406395736\n",
      "Training Loss per 100 steps: 0.35005852580070496\n",
      "Training Accuracy per 100 steps: 84.30277951280449\n",
      "Training Loss per 100 steps: 0.3478687107563019\n",
      "Training Accuracy per 100 steps: 84.40255731922399\n",
      "Training Loss per 100 steps: 0.3445841372013092\n",
      "Training Accuracy per 100 steps: 84.57107162687396\n",
      "Training Loss per 100 steps: 0.3417753279209137\n",
      "Training Accuracy per 100 steps: 84.75802209363493\n",
      "Training Loss per 100 steps: 0.3392239809036255\n",
      "Training Accuracy per 100 steps: 84.87943528235883\n",
      "Training Loss per 100 steps: 0.33704715967178345\n",
      "Training Accuracy per 100 steps: 85.01011423131843\n",
      "Training Loss per 100 steps: 0.33557501435279846\n",
      "Training Accuracy per 100 steps: 85.10052248977738\n",
      "Training Loss per 100 steps: 0.33385130763053894\n",
      "Training Accuracy per 100 steps: 85.2428292046936\n",
      "Training Loss per 100 steps: 0.33171531558036804\n",
      "Training Accuracy per 100 steps: 85.32382340691379\n",
      "The Total Accuracy for Epoch 0: 85.3875\n",
      "Training Loss Epoch: 0.3308812379837036\n",
      "Training Accuracy Epoch: 85.3875\n",
      "Validation Loss Epoch: 0.26523205637931824\n",
      "Validation Accuracy Epoch: 88.76\n",
      "Training Loss per 100 steps: 0.25962573289871216\n",
      "Training Accuracy per 100 steps: 89.60396039603961\n",
      "Training Loss per 100 steps: 0.2351072132587433\n",
      "Training Accuracy per 100 steps: 90.6094527363184\n",
      "Training Loss per 100 steps: 0.22453540563583374\n",
      "Training Accuracy per 100 steps: 91.34136212624584\n",
      "Training Loss per 100 steps: 0.2286592274904251\n",
      "Training Accuracy per 100 steps: 90.99127182044887\n",
      "Training Loss per 100 steps: 0.22947615385055542\n",
      "Training Accuracy per 100 steps: 90.8058882235529\n",
      "Training Loss per 100 steps: 0.23025012016296387\n",
      "Training Accuracy per 100 steps: 90.79658901830283\n",
      "Training Loss per 100 steps: 0.2298593819141388\n",
      "Training Accuracy per 100 steps: 90.75427960057061\n",
      "Training Loss per 100 steps: 0.22596576809883118\n",
      "Training Accuracy per 100 steps: 90.97222222222223\n",
      "Training Loss per 100 steps: 0.22362487018108368\n",
      "Training Accuracy per 100 steps: 91.00998890122086\n",
      "Training Loss per 100 steps: 0.2191668450832367\n",
      "Training Accuracy per 100 steps: 91.14635364635365\n",
      "Training Loss per 100 steps: 0.21835215389728546\n",
      "Training Accuracy per 100 steps: 91.18415077202543\n",
      "Training Loss per 100 steps: 0.22014927864074707\n",
      "Training Accuracy per 100 steps: 91.16361365528726\n",
      "Training Loss per 100 steps: 0.21889078617095947\n",
      "Training Accuracy per 100 steps: 91.19427363566487\n",
      "Training Loss per 100 steps: 0.21869651973247528\n",
      "Training Accuracy per 100 steps: 91.23394004282655\n",
      "Training Loss per 100 steps: 0.2168177366256714\n",
      "Training Accuracy per 100 steps: 91.31828780812792\n",
      "Training Loss per 100 steps: 0.21620863676071167\n",
      "Training Accuracy per 100 steps: 91.27498438475952\n",
      "Training Loss per 100 steps: 0.21534165740013123\n",
      "Training Accuracy per 100 steps: 91.26984126984127\n",
      "Training Loss per 100 steps: 0.21394094824790955\n",
      "Training Accuracy per 100 steps: 91.32079400333149\n",
      "Training Loss per 100 steps: 0.21164454519748688\n",
      "Training Accuracy per 100 steps: 91.4288532351394\n",
      "Training Loss per 100 steps: 0.2118479609489441\n",
      "Training Accuracy per 100 steps: 91.4167916041979\n",
      "Training Loss per 100 steps: 0.21118570864200592\n",
      "Training Accuracy per 100 steps: 91.45942408376963\n",
      "Training Loss per 100 steps: 0.2111232876777649\n",
      "Training Accuracy per 100 steps: 91.45558836892322\n",
      "Training Loss per 100 steps: 0.20993804931640625\n",
      "Training Accuracy per 100 steps: 91.54715341156019\n",
      "Training Loss per 100 steps: 0.20854099094867706\n",
      "Training Accuracy per 100 steps: 91.61026655560184\n",
      "The Total Accuracy for Epoch 1: 91.62\n",
      "Training Loss Epoch: 0.20863354206085205\n",
      "Training Accuracy Epoch: 91.62\n",
      "Validation Loss Epoch: 0.30352210998535156\n",
      "Validation Accuracy Epoch: 88.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-983c66ab15c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidating_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrn_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a0fd502eab2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, training_loader, testing_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Accuracy per 100 steps: {accu_step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/joni/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_val_loss = float('inf')\n",
    "running_trn_loss = float('inf')\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    m, trn_loss, trn_acc, val_loss, val_acc = train(epoch, training_loader, validating_loader)\n",
    "    trn_losses.append(trn_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if (val_loss < running_val_loss) and (val_loss < trn_loss):\n",
    "        running_val_loss = val_loss\n",
    "        running_trn_loss = trn_loss\n",
    "        # Save the best model\n",
    "        output_model_file = f'../models/best-ft-bert-cased-imdb-sentiment-maxlen128-bs16.pt'\n",
    "        model_to_save = m\n",
    "        torch.save(model_to_save, output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/best-ft-bert-cased-painpoint-maxlen35-bs16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, model, tokenizer, device=\"cuda\"):\n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    # tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    tokens = torch.tensor([tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens.to(device), attention_mask=mask.to(device))[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "    return real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import tokenizer from transformers package\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # Load the tokenizer of the \"bert-base-cased\" pretrained model\n",
    "# # See https://huggingface.co/transformers/pretrained_models.html for other models\n",
    "# tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# # The senetence to be encoded\n",
    "# sent = \"Let's learn deep learning!\"\n",
    "\n",
    "# # Encode the sentence\n",
    "# encoded = tz.encode_plus(\n",
    "#     text=sent,  # the sentence to be encoded\n",
    "#     add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#     max_length = 64,  # maximum length of a sentence\n",
    "#     pad_to_max_length=True,  # Add [PAD]s\n",
    "#     return_attention_mask = True,  # Generate the attention mask\n",
    "#     return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "# )\n",
    "\n",
    "# # Get the input IDs and attention mask in tensor format\n",
    "# input_ids = encoded['input_ids']\n",
    "# attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, preds_probas = [],[]\n",
    "for i, row in test_dataset.iterrows():\n",
    "    query = row[\"clean_review\"]\n",
    "    pred = predict(query,model,tokenizer)\n",
    "    preds_probas.append(pred)\n",
    "    if pred >= 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2426,  112],\n",
       "       [ 280, 2182]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_dataset.target.values\n",
    "y_pred = preds\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.16; Precision:95.1176983435048; Recall:88.62713241267262\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc*100}; Precision:{precision*100}; Recall:{recall*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       0.90      0.96      0.93      2538\n",
      "           y       0.95      0.89      0.92      2462\n",
      "\n",
      "    accuracy                           0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.92      0.92      0.92      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"n\",\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
